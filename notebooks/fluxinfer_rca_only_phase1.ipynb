{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FluxInfer RCA (only TSifter Phase1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from tsdr import tsdr\n",
    "from eval import groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meltria import loader\n",
    "\n",
    "metrics_files = !find /datasets/argowf-chaos-rq54b/ -type f -name \"*.json\"\n",
    "dataset_generator = loader.load_dataset_as_generator(metrics_files, target_metric_types={\n",
    "        \"containers\": True,\n",
    "        \"services\": True,\n",
    "        \"nodes\": True,\n",
    "        \"middlewares\": True,\n",
    "    },\n",
    "    num_datapoints=120,\n",
    ")\n",
    "records = [r for rec in dataset_generator for r in rec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_and_reduced_df: list = []\n",
    "for record in records:\n",
    "    # run tsdr\n",
    "    reducer = tsdr.Tsdr(\"residual_integral\", **{\n",
    "        \"step1_residual_integral_threshold\": 20,\n",
    "        \"step1_residual_integral_change_start_point\": False,\n",
    "        \"step1_residual_integral_change_start_point_n_sigma\": 3,\n",
    "        \"step2_clustering_method_name\": \"dbscan\",\n",
    "        \"step2_dbscan_min_pts\": 2,\n",
    "        \"step2_dbscan_dist_type\": 'sbd',\n",
    "        \"step2_dbscan_algorithm\": 'hdbscan',\n",
    "        \"step2_clustering_series_type\": 'raw',\n",
    "        \"step2_clustering_choice_method\": 'medoid',\n",
    "    })\n",
    "    tsdr_stat, clustering_info, anomaly_points = reducer.run(\n",
    "        X=record.data_df,\n",
    "        pk=record.pk,\n",
    "        max_workers=cpu_count(),\n",
    "    )\n",
    "    reduced_df = tsdr_stat[-1][0]\n",
    "    no_clustering_reduced_df = tsdr_stat[-2][0]\n",
    "    record_and_reduced_df.append((record, reduced_df, no_clustering_reduced_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import glob\n",
    "import pathlib\n",
    "\n",
    "\n",
    "def save_tsdr(record, reduced_df, no_clustering_reduced_df):\n",
    "    path = pathlib.Path(f\"../data/tsdr_rq54b/{record.chaos_case_full().replace('/', '_')}\")\n",
    "    path.mkdir()\n",
    "    for obj, name in ((record, \"record\"), (reduced_df, \"reduced_df\"), (no_clustering_reduced_df, \"no_clustering_reduced_df\")):\n",
    "        with open(path / f\"{name}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(obj, f)\n",
    "\n",
    "def load_tsdr():\n",
    "    results = []\n",
    "    parent_path = pathlib.Path(f\"../data/tsdr_rq54b\")\n",
    "    for path in parent_path.iterdir():\n",
    "        with (path / \"record.pkl\").open(\"rb\") as f:\n",
    "            record = pickle.load(f)\n",
    "        with (path / \"reduced_df.pkl\").open(\"rb\") as f:\n",
    "            reduced_df = pickle.load(f)\n",
    "        with (path / \"no_clustering_reduced_df.pkl\").open(\"rb\") as f:\n",
    "            no_clustering_reduced_df = pickle.load(f)\n",
    "        results.append((record, reduced_df, no_clustering_reduced_df))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record, reduced_df, no_clustering_reduced_df in record_and_reduced_df:\n",
    "    save_tsdr(record, reduced_df, no_clustering_reduced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_and_reduced_df = load_tsdr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import diagnoser.metric_node as mn\n",
    "from diagnoser import diag\n",
    "import gc\n",
    "\n",
    "def fisher_z(dm, cm, x, y) -> float:\n",
    "    m = dm.shape[0]\n",
    "    r = cm[x, y]\n",
    "    if 1 - r == 0. or 1 + r == 0.:\n",
    "        r = 1 - 1e-10\n",
    "    zstat = np.sqrt(m - 3) * 0.5 * np.log((1 + r) / (1 - r))\n",
    "    p_val = 2.0 * scipy.stats.norm.sf(np.absolute(zstat))\n",
    "    return p_val\n",
    "\n",
    "def build_wudg(pk, data_df: pd.DataFrame, init_graph_type=\"complete\") -> nx.Graph:\n",
    "    nodes = mn.MetricNodes.from_dataframe(data_df)\n",
    "    g: nx.Graph\n",
    "    match init_graph_type:\n",
    "        case \"complete\":\n",
    "            g = nx.Graph()\n",
    "            for (u, v) in combinations(nodes, 2):\n",
    "                g.add_edge(u, v)\n",
    "        case \"nw_call\":\n",
    "            g = diag.prepare_init_graph(nodes, pk)\n",
    "        case _:\n",
    "            assert False, f\"Unknown init_graph_type: {init_graph_type}\"\n",
    "\n",
    "    dm = data_df.to_numpy()\n",
    "    cm = np.corrcoef(dm.T)\n",
    "    _g = nx.relabel_nodes(g, mapping=nodes.node_to_num, copy=False)\n",
    "    for (u, v) in _g.edges:\n",
    "        p_val = fisher_z(dm, cm, u, v)\n",
    "        _g[u][v]['weight'] = 1 / p_val if p_val != 0.0 else sys.float_info.max\n",
    "\n",
    "    return nx.relabel_nodes(_g, mapping=nodes.num_to_node, copy=False)\n",
    "\n",
    "\n",
    "def build_wudg_and_pagerank(pk, data_df: pd.DataFrame, init_graph_type=\"complete\") -> dict:\n",
    "    g = build_wudg(pk, data_df, init_graph_type)\n",
    "    pr = nx.pagerank(g, alpha=0.85, weight='weight')\n",
    "    del g\n",
    "    gc.collect()\n",
    "    return pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "prs = joblib.Parallel(n_jobs=6)(joblib.delayed(build_wudg_and_pagerank)(record.pk, no_clustering_reduced_df, init_graph_type=\"nw_call\") for record, reduced_df, no_clustering_reduced_df in record_and_reduced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "anomaly_case_sizes = len(prs)\n",
    "top_k_set = range(1, 11)\n",
    "ac_k: dict[int, float] = {k: 0.0 for k in top_k_set}\n",
    "rank_by_case: dict[str, list[int]] = defaultdict(list)\n",
    "print(len(prs), len(record_and_reduced_df))\n",
    "for pr, (record, _, _) in zip(prs, record_and_reduced_df):\n",
    "    chaos_service: str = record.chaos_comp().removesuffix(\"-service\").removesuffix(\"-mongo\")\n",
    "    ranked_metric_to_score: list[tuple[mn.MetricNode, float]] = sorted(pr.items(), reverse=True, key=lambda x: x[1])\n",
    "    rank: int = sorted([i+1 for i, (m, _) in enumerate(ranked_metric_to_score) if m.comp.startswith(chaos_service)])[0]\n",
    "    print(f\"rank: {rank}, {record.chaos_case_full()}\")\n",
    "    rank_by_case[record.chaos_type()].append(rank)\n",
    "    # plt.plot(reduced_df[str(cm)].to_numpy())\n",
    "\n",
    "for k in top_k_set:\n",
    "    ranks = chain.from_iterable(rank_by_case.values())\n",
    "    ac_k[k] = sum([1 if rank <= k else 0 for rank in ranks]) / anomaly_case_sizes\n",
    "display(\"AC@K\", ac_k)\n",
    "\n",
    "avg_k = {}\n",
    "for k in top_k_set:\n",
    "    avg_k[k] = sum([ac_k[j] for j in range(1, k+1)]) / k\n",
    "display(\"AVG@k\", avg_k)\n",
    "\n",
    "for case, ranks in rank_by_case.items():\n",
    "    _ac_k, _avg_k = {}, {}\n",
    "    for k in top_k_set:\n",
    "        _ac_k[k] = sum([1 if rank <= k else 0 for rank in ranks]) / len(ranks)\n",
    "        _avg_k[k] = sum([_ac_k[j] for j in range(1, k+1)]) / k\n",
    "    display(f\"{case}:AC@K\", _ac_k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c0f02f5f97634d426ffcfa502db37ef392cddba0a927ded2fc10600c3b8bead5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
