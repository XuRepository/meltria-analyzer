{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN based learning anomaly patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"font.family\"] = \"DejaVu Sans\"\n",
    "plt.rcParams[\"font.size\"] = 7\n",
    "plt.rcParams['xtick.labelsize'] = 9\n",
    "plt.rcParams['ytick.labelsize'] = 9\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'\n",
    "plt.rcParams['axes.linewidth'] = 1.0\n",
    "plt.rcParams['axes.grid'] = True\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "class TSDatasetAnomalyPatterns(Dataset):\n",
    "    def __init__(self, path: pathlib.Path):\n",
    "        super(TSDatasetAnomalyPatterns, self).__init__()\n",
    "        self.df = pd.read_json(str(path), orient='records', lines=True)\n",
    "        self.time_series = torch.tensor([\n",
    "            [minmax_scale(li, feature_range=(0, 1)) for li in self.df.loc[:, \"time_series\"].values]\n",
    "        ], dtype=torch.float32)  # use float64 to avoid error\n",
    "\n",
    "        self.joined_categories = self.df.loc[:, [\"anomaly_pattern\", \"anomaly_position\"]].apply(\n",
    "            lambda x: \"/\".join(x.dropna().astype(str).values), axis=1\n",
    "        ).to_numpy()\n",
    "\n",
    "        # string labels to int labels\n",
    "        self.label_to_category = {i: v for i, v in enumerate(np.unique(self.joined_categories))}\n",
    "        category_to_label = {v: k for k, v in self.label_to_category.items()}\n",
    "        self.labels = torch.tensor([\n",
    "            category_to_label[c] for c in self.joined_categories\n",
    "        ], dtype=torch.int64)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        category = self.label_to_category[label.item()]\n",
    "        return self.time_series[:, idx], label, category\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TSDatasetAnomalyPatterns(pathlib.Path(\"../samples/tsdr_anomaly_patterns/labeled_tsdr_anomaly_patterns_20221202-024759.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "836"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "357"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.utils.data\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset=dataset, lengths=[0.7, 0.3], generator=torch.Generator().manual_seed(42))\n",
    "display(len(train_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref. https://github.com/pytorch/examples/blob/f82f5626b6432b8d0b08d58cc91f3bdbb355a772/mnist/main.py\n",
    "\n",
    "class CNN1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN1d, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=5)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5)\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=5)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(20992, 64)  # the number of datapoints in a metric = 60/15 * 45 (45min * 15sec interval)\n",
    "        self.fc2 = nn.Linear(64, 28)  # The number of class is 13 (chaos types) * 2 (anomaly position) + 2 (normal and unknown)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, kernel_size=5, stride=2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)  # Is this necessary?\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(log_interval, model, device, train_loader, optimizer, loss_fn, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target, category) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader, loss_fn):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target, category in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/836 (0%)]\tLoss: 3.335617\n",
      "Train Epoch: 1 [40/836 (5%)]\tLoss: 2.514638\n",
      "Train Epoch: 1 [80/836 (10%)]\tLoss: 3.105530\n",
      "Train Epoch: 1 [120/836 (14%)]\tLoss: 3.201548\n",
      "Train Epoch: 1 [160/836 (19%)]\tLoss: 3.105174\n",
      "Train Epoch: 1 [200/836 (24%)]\tLoss: 1.882931\n",
      "Train Epoch: 1 [240/836 (29%)]\tLoss: 2.948489\n",
      "Train Epoch: 1 [280/836 (33%)]\tLoss: 2.987756\n",
      "Train Epoch: 1 [320/836 (38%)]\tLoss: 1.340817\n",
      "Train Epoch: 1 [360/836 (43%)]\tLoss: 2.354084\n",
      "Train Epoch: 1 [400/836 (48%)]\tLoss: 1.948641\n",
      "Train Epoch: 1 [440/836 (53%)]\tLoss: 2.937404\n",
      "Train Epoch: 1 [480/836 (57%)]\tLoss: 1.775400\n",
      "Train Epoch: 1 [520/836 (62%)]\tLoss: 1.575277\n",
      "Train Epoch: 1 [560/836 (67%)]\tLoss: 2.642988\n",
      "Train Epoch: 1 [600/836 (72%)]\tLoss: 1.491587\n",
      "Train Epoch: 1 [640/836 (77%)]\tLoss: 1.834068\n",
      "Train Epoch: 1 [680/836 (81%)]\tLoss: 1.688357\n",
      "Train Epoch: 1 [720/836 (86%)]\tLoss: 1.516830\n",
      "Train Epoch: 1 [760/836 (91%)]\tLoss: 2.395550\n",
      "Train Epoch: 1 [800/836 (96%)]\tLoss: 1.712293\n",
      "\n",
      "Test set: Average loss: 0.4591, Accuracy: 163/357 (46%)\n",
      "\n",
      "Train Epoch: 2 [0/836 (0%)]\tLoss: 1.275394\n",
      "Train Epoch: 2 [40/836 (5%)]\tLoss: 1.867304\n",
      "Train Epoch: 2 [80/836 (10%)]\tLoss: 2.281573\n",
      "Train Epoch: 2 [120/836 (14%)]\tLoss: 0.873073\n",
      "Train Epoch: 2 [160/836 (19%)]\tLoss: 1.540093\n",
      "Train Epoch: 2 [200/836 (24%)]\tLoss: 1.187320\n",
      "Train Epoch: 2 [240/836 (29%)]\tLoss: 1.490917\n",
      "Train Epoch: 2 [280/836 (33%)]\tLoss: 1.671135\n",
      "Train Epoch: 2 [320/836 (38%)]\tLoss: 2.062005\n",
      "Train Epoch: 2 [360/836 (43%)]\tLoss: 1.716962\n",
      "Train Epoch: 2 [400/836 (48%)]\tLoss: 1.719732\n",
      "Train Epoch: 2 [440/836 (53%)]\tLoss: 1.811591\n",
      "Train Epoch: 2 [480/836 (57%)]\tLoss: 0.925984\n",
      "Train Epoch: 2 [520/836 (62%)]\tLoss: 1.373291\n",
      "Train Epoch: 2 [560/836 (67%)]\tLoss: 1.806606\n",
      "Train Epoch: 2 [600/836 (72%)]\tLoss: 1.456566\n",
      "Train Epoch: 2 [640/836 (77%)]\tLoss: 1.836035\n",
      "Train Epoch: 2 [680/836 (81%)]\tLoss: 0.627225\n",
      "Train Epoch: 2 [720/836 (86%)]\tLoss: 2.339213\n",
      "Train Epoch: 2 [760/836 (91%)]\tLoss: 1.834822\n",
      "Train Epoch: 2 [800/836 (96%)]\tLoss: 1.601392\n",
      "\n",
      "Test set: Average loss: 0.3527, Accuracy: 198/357 (55%)\n",
      "\n",
      "Train Epoch: 3 [0/836 (0%)]\tLoss: 1.705196\n",
      "Train Epoch: 3 [40/836 (5%)]\tLoss: 1.364864\n",
      "Train Epoch: 3 [80/836 (10%)]\tLoss: 2.345716\n",
      "Train Epoch: 3 [120/836 (14%)]\tLoss: 1.739594\n",
      "Train Epoch: 3 [160/836 (19%)]\tLoss: 2.083339\n",
      "Train Epoch: 3 [200/836 (24%)]\tLoss: 1.781089\n",
      "Train Epoch: 3 [240/836 (29%)]\tLoss: 3.967534\n",
      "Train Epoch: 3 [280/836 (33%)]\tLoss: 1.179553\n",
      "Train Epoch: 3 [320/836 (38%)]\tLoss: 1.759310\n",
      "Train Epoch: 3 [360/836 (43%)]\tLoss: 1.618369\n",
      "Train Epoch: 3 [400/836 (48%)]\tLoss: 0.334773\n",
      "Train Epoch: 3 [440/836 (53%)]\tLoss: 1.855592\n",
      "Train Epoch: 3 [480/836 (57%)]\tLoss: 1.523593\n",
      "Train Epoch: 3 [520/836 (62%)]\tLoss: 0.943768\n",
      "Train Epoch: 3 [560/836 (67%)]\tLoss: 1.062358\n",
      "Train Epoch: 3 [600/836 (72%)]\tLoss: 0.532780\n",
      "Train Epoch: 3 [640/836 (77%)]\tLoss: 0.752616\n",
      "Train Epoch: 3 [680/836 (81%)]\tLoss: 1.124984\n",
      "Train Epoch: 3 [720/836 (86%)]\tLoss: 0.708506\n",
      "Train Epoch: 3 [760/836 (91%)]\tLoss: 1.274596\n",
      "Train Epoch: 3 [800/836 (96%)]\tLoss: 1.823886\n",
      "\n",
      "Test set: Average loss: 0.2978, Accuracy: 216/357 (61%)\n",
      "\n",
      "Train Epoch: 4 [0/836 (0%)]\tLoss: 2.573840\n",
      "Train Epoch: 4 [40/836 (5%)]\tLoss: 1.239396\n",
      "Train Epoch: 4 [80/836 (10%)]\tLoss: 1.311406\n",
      "Train Epoch: 4 [120/836 (14%)]\tLoss: 1.855560\n",
      "Train Epoch: 4 [160/836 (19%)]\tLoss: 1.138590\n",
      "Train Epoch: 4 [200/836 (24%)]\tLoss: 1.240161\n",
      "Train Epoch: 4 [240/836 (29%)]\tLoss: 1.594461\n",
      "Train Epoch: 4 [280/836 (33%)]\tLoss: 1.011239\n",
      "Train Epoch: 4 [320/836 (38%)]\tLoss: 0.459165\n",
      "Train Epoch: 4 [360/836 (43%)]\tLoss: 0.293471\n",
      "Train Epoch: 4 [400/836 (48%)]\tLoss: 0.958195\n",
      "Train Epoch: 4 [440/836 (53%)]\tLoss: 1.564803\n",
      "Train Epoch: 4 [480/836 (57%)]\tLoss: 0.528186\n",
      "Train Epoch: 4 [520/836 (62%)]\tLoss: 1.239772\n",
      "Train Epoch: 4 [560/836 (67%)]\tLoss: 2.261894\n",
      "Train Epoch: 4 [600/836 (72%)]\tLoss: 2.014152\n",
      "Train Epoch: 4 [640/836 (77%)]\tLoss: 2.861294\n",
      "Train Epoch: 4 [680/836 (81%)]\tLoss: 0.962475\n",
      "Train Epoch: 4 [720/836 (86%)]\tLoss: 1.069508\n",
      "Train Epoch: 4 [760/836 (91%)]\tLoss: 0.760709\n",
      "Train Epoch: 4 [800/836 (96%)]\tLoss: 2.026927\n",
      "\n",
      "Test set: Average loss: 0.2662, Accuracy: 230/357 (64%)\n",
      "\n",
      "Train Epoch: 5 [0/836 (0%)]\tLoss: 0.562562\n",
      "Train Epoch: 5 [40/836 (5%)]\tLoss: 0.392057\n",
      "Train Epoch: 5 [80/836 (10%)]\tLoss: 2.445322\n",
      "Train Epoch: 5 [120/836 (14%)]\tLoss: 1.033679\n",
      "Train Epoch: 5 [160/836 (19%)]\tLoss: 0.989377\n",
      "Train Epoch: 5 [200/836 (24%)]\tLoss: 1.943811\n",
      "Train Epoch: 5 [240/836 (29%)]\tLoss: 1.223451\n",
      "Train Epoch: 5 [280/836 (33%)]\tLoss: 2.329103\n",
      "Train Epoch: 5 [320/836 (38%)]\tLoss: 0.451822\n",
      "Train Epoch: 5 [360/836 (43%)]\tLoss: 0.428725\n",
      "Train Epoch: 5 [400/836 (48%)]\tLoss: 1.746533\n",
      "Train Epoch: 5 [440/836 (53%)]\tLoss: 0.702672\n",
      "Train Epoch: 5 [480/836 (57%)]\tLoss: 1.009320\n",
      "Train Epoch: 5 [520/836 (62%)]\tLoss: 0.871247\n",
      "Train Epoch: 5 [560/836 (67%)]\tLoss: 2.204939\n",
      "Train Epoch: 5 [600/836 (72%)]\tLoss: 0.743048\n",
      "Train Epoch: 5 [640/836 (77%)]\tLoss: 2.021802\n",
      "Train Epoch: 5 [680/836 (81%)]\tLoss: 1.067626\n",
      "Train Epoch: 5 [720/836 (86%)]\tLoss: 1.556583\n",
      "Train Epoch: 5 [760/836 (91%)]\tLoss: 0.485954\n",
      "Train Epoch: 5 [800/836 (96%)]\tLoss: 2.799656\n",
      "\n",
      "Test set: Average loss: 0.2491, Accuracy: 242/357 (68%)\n",
      "\n",
      "Train Epoch: 6 [0/836 (0%)]\tLoss: 0.828502\n",
      "Train Epoch: 6 [40/836 (5%)]\tLoss: 1.377994\n",
      "Train Epoch: 6 [80/836 (10%)]\tLoss: 0.793314\n",
      "Train Epoch: 6 [120/836 (14%)]\tLoss: 1.067357\n",
      "Train Epoch: 6 [160/836 (19%)]\tLoss: 1.026428\n",
      "Train Epoch: 6 [200/836 (24%)]\tLoss: 0.670116\n",
      "Train Epoch: 6 [240/836 (29%)]\tLoss: 0.622097\n",
      "Train Epoch: 6 [280/836 (33%)]\tLoss: 0.744871\n",
      "Train Epoch: 6 [320/836 (38%)]\tLoss: 1.409060\n",
      "Train Epoch: 6 [360/836 (43%)]\tLoss: 0.528972\n",
      "Train Epoch: 6 [400/836 (48%)]\tLoss: 0.586734\n",
      "Train Epoch: 6 [440/836 (53%)]\tLoss: 1.212537\n",
      "Train Epoch: 6 [480/836 (57%)]\tLoss: 0.789658\n",
      "Train Epoch: 6 [520/836 (62%)]\tLoss: 1.524139\n",
      "Train Epoch: 6 [560/836 (67%)]\tLoss: 0.832842\n",
      "Train Epoch: 6 [600/836 (72%)]\tLoss: 1.153496\n",
      "Train Epoch: 6 [640/836 (77%)]\tLoss: 1.753291\n",
      "Train Epoch: 6 [680/836 (81%)]\tLoss: 1.431467\n",
      "Train Epoch: 6 [720/836 (86%)]\tLoss: 1.367857\n",
      "Train Epoch: 6 [760/836 (91%)]\tLoss: 1.317349\n",
      "Train Epoch: 6 [800/836 (96%)]\tLoss: 1.040667\n",
      "\n",
      "Test set: Average loss: 0.2368, Accuracy: 250/357 (70%)\n",
      "\n",
      "Train Epoch: 7 [0/836 (0%)]\tLoss: 1.400784\n",
      "Train Epoch: 7 [40/836 (5%)]\tLoss: 0.859616\n",
      "Train Epoch: 7 [80/836 (10%)]\tLoss: 0.497103\n",
      "Train Epoch: 7 [120/836 (14%)]\tLoss: 0.920506\n",
      "Train Epoch: 7 [160/836 (19%)]\tLoss: 1.258228\n",
      "Train Epoch: 7 [200/836 (24%)]\tLoss: 1.140408\n",
      "Train Epoch: 7 [240/836 (29%)]\tLoss: 0.811197\n",
      "Train Epoch: 7 [280/836 (33%)]\tLoss: 1.147058\n",
      "Train Epoch: 7 [320/836 (38%)]\tLoss: 0.487954\n",
      "Train Epoch: 7 [360/836 (43%)]\tLoss: 0.223248\n",
      "Train Epoch: 7 [400/836 (48%)]\tLoss: 1.629070\n",
      "Train Epoch: 7 [440/836 (53%)]\tLoss: 0.765903\n",
      "Train Epoch: 7 [480/836 (57%)]\tLoss: 1.207704\n",
      "Train Epoch: 7 [520/836 (62%)]\tLoss: 0.192205\n",
      "Train Epoch: 7 [560/836 (67%)]\tLoss: 1.255948\n",
      "Train Epoch: 7 [600/836 (72%)]\tLoss: 1.114586\n",
      "Train Epoch: 7 [640/836 (77%)]\tLoss: 0.936180\n",
      "Train Epoch: 7 [680/836 (81%)]\tLoss: 1.222779\n",
      "Train Epoch: 7 [720/836 (86%)]\tLoss: 1.103809\n",
      "Train Epoch: 7 [760/836 (91%)]\tLoss: 0.469603\n",
      "Train Epoch: 7 [800/836 (96%)]\tLoss: 1.244835\n",
      "\n",
      "Test set: Average loss: 0.2209, Accuracy: 252/357 (71%)\n",
      "\n",
      "Train Epoch: 8 [0/836 (0%)]\tLoss: 0.998812\n",
      "Train Epoch: 8 [40/836 (5%)]\tLoss: 1.046848\n",
      "Train Epoch: 8 [80/836 (10%)]\tLoss: 0.982732\n",
      "Train Epoch: 8 [120/836 (14%)]\tLoss: 0.330273\n",
      "Train Epoch: 8 [160/836 (19%)]\tLoss: 1.269639\n",
      "Train Epoch: 8 [200/836 (24%)]\tLoss: 0.166154\n",
      "Train Epoch: 8 [240/836 (29%)]\tLoss: 0.550459\n",
      "Train Epoch: 8 [280/836 (33%)]\tLoss: 0.729319\n",
      "Train Epoch: 8 [320/836 (38%)]\tLoss: 1.505254\n",
      "Train Epoch: 8 [360/836 (43%)]\tLoss: 0.732591\n",
      "Train Epoch: 8 [400/836 (48%)]\tLoss: 1.101926\n",
      "Train Epoch: 8 [440/836 (53%)]\tLoss: 0.458269\n",
      "Train Epoch: 8 [480/836 (57%)]\tLoss: 1.493006\n",
      "Train Epoch: 8 [520/836 (62%)]\tLoss: 1.694169\n",
      "Train Epoch: 8 [560/836 (67%)]\tLoss: 0.900737\n",
      "Train Epoch: 8 [600/836 (72%)]\tLoss: 0.623467\n",
      "Train Epoch: 8 [640/836 (77%)]\tLoss: 1.422371\n",
      "Train Epoch: 8 [680/836 (81%)]\tLoss: 0.942248\n",
      "Train Epoch: 8 [720/836 (86%)]\tLoss: 2.090064\n",
      "Train Epoch: 8 [760/836 (91%)]\tLoss: 0.193824\n",
      "Train Epoch: 8 [800/836 (96%)]\tLoss: 0.227966\n",
      "\n",
      "Test set: Average loss: 0.2211, Accuracy: 255/357 (71%)\n",
      "\n",
      "Train Epoch: 9 [0/836 (0%)]\tLoss: 0.743310\n",
      "Train Epoch: 9 [40/836 (5%)]\tLoss: 1.140918\n",
      "Train Epoch: 9 [80/836 (10%)]\tLoss: 0.959662\n",
      "Train Epoch: 9 [120/836 (14%)]\tLoss: 0.748688\n",
      "Train Epoch: 9 [160/836 (19%)]\tLoss: 1.683576\n",
      "Train Epoch: 9 [200/836 (24%)]\tLoss: 2.828873\n",
      "Train Epoch: 9 [240/836 (29%)]\tLoss: 0.646713\n",
      "Train Epoch: 9 [280/836 (33%)]\tLoss: 1.306234\n",
      "Train Epoch: 9 [320/836 (38%)]\tLoss: 2.934363\n",
      "Train Epoch: 9 [360/836 (43%)]\tLoss: 0.989308\n",
      "Train Epoch: 9 [400/836 (48%)]\tLoss: 0.545788\n",
      "Train Epoch: 9 [440/836 (53%)]\tLoss: 0.494226\n",
      "Train Epoch: 9 [480/836 (57%)]\tLoss: 0.551073\n",
      "Train Epoch: 9 [520/836 (62%)]\tLoss: 0.824520\n",
      "Train Epoch: 9 [560/836 (67%)]\tLoss: 1.219981\n",
      "Train Epoch: 9 [600/836 (72%)]\tLoss: 1.649970\n",
      "Train Epoch: 9 [640/836 (77%)]\tLoss: 0.598560\n",
      "Train Epoch: 9 [680/836 (81%)]\tLoss: 0.593846\n",
      "Train Epoch: 9 [720/836 (86%)]\tLoss: 0.954001\n",
      "Train Epoch: 9 [760/836 (91%)]\tLoss: 0.130639\n",
      "Train Epoch: 9 [800/836 (96%)]\tLoss: 0.459232\n",
      "\n",
      "Test set: Average loss: 0.2207, Accuracy: 259/357 (73%)\n",
      "\n",
      "Train Epoch: 10 [0/836 (0%)]\tLoss: 1.615639\n",
      "Train Epoch: 10 [40/836 (5%)]\tLoss: 2.098179\n",
      "Train Epoch: 10 [80/836 (10%)]\tLoss: 0.703682\n",
      "Train Epoch: 10 [120/836 (14%)]\tLoss: 0.520979\n",
      "Train Epoch: 10 [160/836 (19%)]\tLoss: 0.541144\n",
      "Train Epoch: 10 [200/836 (24%)]\tLoss: 0.835816\n",
      "Train Epoch: 10 [240/836 (29%)]\tLoss: 0.989789\n",
      "Train Epoch: 10 [280/836 (33%)]\tLoss: 0.324554\n",
      "Train Epoch: 10 [320/836 (38%)]\tLoss: 1.551357\n",
      "Train Epoch: 10 [360/836 (43%)]\tLoss: 0.387530\n",
      "Train Epoch: 10 [400/836 (48%)]\tLoss: 0.888506\n",
      "Train Epoch: 10 [440/836 (53%)]\tLoss: 0.688135\n",
      "Train Epoch: 10 [480/836 (57%)]\tLoss: 2.177100\n",
      "Train Epoch: 10 [520/836 (62%)]\tLoss: 1.336842\n",
      "Train Epoch: 10 [560/836 (67%)]\tLoss: 1.060094\n",
      "Train Epoch: 10 [600/836 (72%)]\tLoss: 0.867837\n",
      "Train Epoch: 10 [640/836 (77%)]\tLoss: 0.672278\n",
      "Train Epoch: 10 [680/836 (81%)]\tLoss: 0.274885\n",
      "Train Epoch: 10 [720/836 (86%)]\tLoss: 0.773581\n",
      "Train Epoch: 10 [760/836 (91%)]\tLoss: 0.543314\n",
      "Train Epoch: 10 [800/836 (96%)]\tLoss: 2.052550\n",
      "\n",
      "Test set: Average loss: 0.2133, Accuracy: 261/357 (73%)\n",
      "\n",
      "Train Epoch: 11 [0/836 (0%)]\tLoss: 0.616082\n",
      "Train Epoch: 11 [40/836 (5%)]\tLoss: 0.298929\n",
      "Train Epoch: 11 [80/836 (10%)]\tLoss: 0.777525\n",
      "Train Epoch: 11 [120/836 (14%)]\tLoss: 0.753417\n",
      "Train Epoch: 11 [160/836 (19%)]\tLoss: 0.637920\n",
      "Train Epoch: 11 [200/836 (24%)]\tLoss: 0.751875\n",
      "Train Epoch: 11 [240/836 (29%)]\tLoss: 1.203823\n",
      "Train Epoch: 11 [280/836 (33%)]\tLoss: 0.419223\n",
      "Train Epoch: 11 [320/836 (38%)]\tLoss: 1.377920\n",
      "Train Epoch: 11 [360/836 (43%)]\tLoss: 0.751078\n",
      "Train Epoch: 11 [400/836 (48%)]\tLoss: 0.590912\n",
      "Train Epoch: 11 [440/836 (53%)]\tLoss: 0.384725\n",
      "Train Epoch: 11 [480/836 (57%)]\tLoss: 1.215715\n",
      "Train Epoch: 11 [520/836 (62%)]\tLoss: 1.322905\n",
      "Train Epoch: 11 [560/836 (67%)]\tLoss: 0.795413\n",
      "Train Epoch: 11 [600/836 (72%)]\tLoss: 1.511723\n",
      "Train Epoch: 11 [640/836 (77%)]\tLoss: 0.768509\n",
      "Train Epoch: 11 [680/836 (81%)]\tLoss: 0.536513\n",
      "Train Epoch: 11 [720/836 (86%)]\tLoss: 0.585496\n",
      "Train Epoch: 11 [760/836 (91%)]\tLoss: 1.166884\n",
      "Train Epoch: 11 [800/836 (96%)]\tLoss: 0.328594\n",
      "\n",
      "Test set: Average loss: 0.2195, Accuracy: 262/357 (73%)\n",
      "\n",
      "Train Epoch: 12 [0/836 (0%)]\tLoss: 1.313535\n",
      "Train Epoch: 12 [40/836 (5%)]\tLoss: 1.473623\n",
      "Train Epoch: 12 [80/836 (10%)]\tLoss: 1.892728\n",
      "Train Epoch: 12 [120/836 (14%)]\tLoss: 0.485049\n",
      "Train Epoch: 12 [160/836 (19%)]\tLoss: 0.955611\n",
      "Train Epoch: 12 [200/836 (24%)]\tLoss: 0.838096\n",
      "Train Epoch: 12 [240/836 (29%)]\tLoss: 0.852488\n",
      "Train Epoch: 12 [280/836 (33%)]\tLoss: 1.210890\n",
      "Train Epoch: 12 [320/836 (38%)]\tLoss: 1.126839\n",
      "Train Epoch: 12 [360/836 (43%)]\tLoss: 1.476242\n",
      "Train Epoch: 12 [400/836 (48%)]\tLoss: 0.699377\n",
      "Train Epoch: 12 [440/836 (53%)]\tLoss: 1.239176\n",
      "Train Epoch: 12 [480/836 (57%)]\tLoss: 0.225408\n",
      "Train Epoch: 12 [520/836 (62%)]\tLoss: 1.005254\n",
      "Train Epoch: 12 [560/836 (67%)]\tLoss: 0.540249\n",
      "Train Epoch: 12 [600/836 (72%)]\tLoss: 0.437354\n",
      "Train Epoch: 12 [640/836 (77%)]\tLoss: 0.505900\n",
      "Train Epoch: 12 [680/836 (81%)]\tLoss: 1.216412\n",
      "Train Epoch: 12 [720/836 (86%)]\tLoss: 0.904031\n",
      "Train Epoch: 12 [760/836 (91%)]\tLoss: 1.018896\n",
      "Train Epoch: 12 [800/836 (96%)]\tLoss: 1.071714\n",
      "\n",
      "Test set: Average loss: 0.2170, Accuracy: 263/357 (74%)\n",
      "\n",
      "Train Epoch: 13 [0/836 (0%)]\tLoss: 0.134962\n",
      "Train Epoch: 13 [40/836 (5%)]\tLoss: 0.853509\n",
      "Train Epoch: 13 [80/836 (10%)]\tLoss: 0.906432\n",
      "Train Epoch: 13 [120/836 (14%)]\tLoss: 1.209963\n",
      "Train Epoch: 13 [160/836 (19%)]\tLoss: 1.483329\n",
      "Train Epoch: 13 [200/836 (24%)]\tLoss: 0.931532\n",
      "Train Epoch: 13 [240/836 (29%)]\tLoss: 1.075986\n",
      "Train Epoch: 13 [280/836 (33%)]\tLoss: 0.434211\n",
      "Train Epoch: 13 [320/836 (38%)]\tLoss: 0.904400\n",
      "Train Epoch: 13 [360/836 (43%)]\tLoss: 1.363509\n",
      "Train Epoch: 13 [400/836 (48%)]\tLoss: 0.959403\n",
      "Train Epoch: 13 [440/836 (53%)]\tLoss: 0.311002\n",
      "Train Epoch: 13 [480/836 (57%)]\tLoss: 0.439915\n",
      "Train Epoch: 13 [520/836 (62%)]\tLoss: 0.524453\n",
      "Train Epoch: 13 [560/836 (67%)]\tLoss: 0.769674\n",
      "Train Epoch: 13 [600/836 (72%)]\tLoss: 1.781909\n",
      "Train Epoch: 13 [640/836 (77%)]\tLoss: 0.940769\n",
      "Train Epoch: 13 [680/836 (81%)]\tLoss: 1.933706\n",
      "Train Epoch: 13 [720/836 (86%)]\tLoss: 1.177062\n",
      "Train Epoch: 13 [760/836 (91%)]\tLoss: 0.368716\n",
      "Train Epoch: 13 [800/836 (96%)]\tLoss: 1.060354\n",
      "\n",
      "Test set: Average loss: 0.2116, Accuracy: 261/357 (73%)\n",
      "\n",
      "Train Epoch: 14 [0/836 (0%)]\tLoss: 1.844075\n",
      "Train Epoch: 14 [40/836 (5%)]\tLoss: 1.181912\n",
      "Train Epoch: 14 [80/836 (10%)]\tLoss: 2.566980\n",
      "Train Epoch: 14 [120/836 (14%)]\tLoss: 0.628138\n",
      "Train Epoch: 14 [160/836 (19%)]\tLoss: 0.385793\n",
      "Train Epoch: 14 [200/836 (24%)]\tLoss: 0.940839\n",
      "Train Epoch: 14 [240/836 (29%)]\tLoss: 0.510276\n",
      "Train Epoch: 14 [280/836 (33%)]\tLoss: 1.463365\n",
      "Train Epoch: 14 [320/836 (38%)]\tLoss: 0.874684\n",
      "Train Epoch: 14 [360/836 (43%)]\tLoss: 0.808867\n",
      "Train Epoch: 14 [400/836 (48%)]\tLoss: 0.871748\n",
      "Train Epoch: 14 [440/836 (53%)]\tLoss: 0.819422\n",
      "Train Epoch: 14 [480/836 (57%)]\tLoss: 0.877032\n",
      "Train Epoch: 14 [520/836 (62%)]\tLoss: 0.840010\n",
      "Train Epoch: 14 [560/836 (67%)]\tLoss: 0.858318\n",
      "Train Epoch: 14 [600/836 (72%)]\tLoss: 0.957344\n",
      "Train Epoch: 14 [640/836 (77%)]\tLoss: 0.604349\n",
      "Train Epoch: 14 [680/836 (81%)]\tLoss: 1.422858\n",
      "Train Epoch: 14 [720/836 (86%)]\tLoss: 0.910693\n",
      "Train Epoch: 14 [760/836 (91%)]\tLoss: 1.217857\n",
      "Train Epoch: 14 [800/836 (96%)]\tLoss: 0.784483\n",
      "\n",
      "Test set: Average loss: 0.2131, Accuracy: 260/357 (73%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = CNN1d().to(device)\n",
    "learning_rate = 0.001\n",
    "epochs = 14\n",
    "batch_size = 8\n",
    "test_batch_size = 8\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(10, model, device, train_dataloader, optimizer, loss_fn, epoch)\n",
    "    test(model, device, test_dataloader, loss_fn)\n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c0f02f5f97634d426ffcfa502db37ef392cddba0a927ded2fc10600c3b8bead5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
