{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of clustering (shape-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"font.family\"] = \"DejaVu Sans\"\n",
    "plt.rcParams[\"font.size\"] = 15\n",
    "plt.rcParams['xtick.labelsize'] = 14\n",
    "plt.rcParams['ytick.labelsize'] = 16\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'\n",
    "plt.rcParams['axes.linewidth'] = 1.0\n",
    "plt.rcParams['axes.grid'] = True\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from tsdr import tsdr\n",
    "from eval import groundtruth\n",
    "from meltria import loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_files = !find /datasets/argowf-chaos-rq54b/ -type f -name \"*.json\" | tail -n1\n",
    "dataset_generator = loader.load_dataset_as_generator(metrics_files, target_metric_types={\n",
    "        \"containers\": True,\n",
    "        \"services\": True,\n",
    "        \"nodes\": True,\n",
    "        \"middlewares\": True,\n",
    "    },\n",
    "    num_datapoints=120,\n",
    ")\n",
    "records = [r for rec in dataset_generator for r in rec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = records[0]\n",
    "reducer = tsdr.Tsdr(\"residual_integral\", **{\n",
    "    \"step1_residual_integral_threshold\": 20,\n",
    "    \"step1_residual_integral_change_start_point\": False,\n",
    "    \"step1_residual_integral_change_start_point_n_sigma\": 3,\n",
    "    \"step2_clustering_method_name\": \"dbscan\",\n",
    "    \"step2_dbscan_min_pts\": 2,\n",
    "    \"step2_dbscan_dist_type\": 'sbd',\n",
    "    \"step2_dbscan_algorithm\": 'hdbscan',\n",
    "    \"step2_clustering_series_type\": 'raw',\n",
    "    \"step2_clustering_choice_method\": 'medoid',\n",
    "})\n",
    "tsdr_stat, clustering_info, anomaly_points = reducer.run(\n",
    "    X=record.data_df,\n",
    "    pk=record.pk,\n",
    "    max_workers=cpu_count(),\n",
    ")\n",
    "reduced_df = tsdr_stat[-1][0]\n",
    "anomalous_metrics_df = tsdr_stat[-2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause_service: str = record.pk.get_service_by_container(record.chaos_comp())\n",
    "grouped_metrics_by_service = record.pk.group_metrics_by_service(list(anomalous_metrics_df.columns))\n",
    "cause_service_metrics: list[str] = grouped_metrics_by_service[cause_service]\n",
    "anomalous_cause_service_metrics_df = anomalous_metrics_df[cause_service_metrics]\n",
    "anomalous_cause_service_metrics_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import jsonlines\n",
    "import glob\n",
    "\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DTW + KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tslearn.neighbors import KNeighborsTimeSeries\n",
    "from tslearn.metrics import dtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.interpolate\n",
    "\n",
    "# NUM_DATAPOINTS = 120\n",
    "\n",
    "# def interp1d(_ts: np.ndarray) -> np.ndarray:\n",
    "#     ts = _ts[~np.isnan(_ts)]\n",
    "#     return scipy.interpolate.interp1d(\n",
    "#         x=np.arange(NUM_DATAPOINTS - len(ts), NUM_DATAPOINTS), y=ts, kind=\"cubic\", fill_value=\"extrapolate\",\n",
    "#     )(np.arange(0, NUM_DATAPOINTS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsdr.clustering.sbd import sbd\n",
    "\n",
    "def build_clustered_metrics(X: pd.DataFrame, k: int = 5):\n",
    "    # remove metrics that have nan.\n",
    "    # _ts = ts.apply(lambda x: pd.Series(interp1d(x.to_numpy())), axis=0)\n",
    "    _X = X.loc[:, X.apply(lambda x: not x.isna().any())]\n",
    "    # def distance(x, y) -> float:\n",
    "    #     return fastdtw(x, y)[0]\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, metric=dtw).fit(_X.values.T)\n",
    "    idx_arrays = nbrs.kneighbors(return_distance=False)\n",
    "    return [_X.columns[idx].to_list() for idx in idx_arrays]\n",
    "\n",
    "def find_similar_metrics(X: pd.DataFrame, col: str, k: int = 5) -> pd.DataFrame:\n",
    "    base_x = X.loc[:, col].to_numpy()\n",
    "    _X: pd.DataFrame = X.loc[:, X.columns!=col]\n",
    "    topk = _X.agg(lambda y: dtw(base_x, y.to_numpy())).T.sort_values().head(k)\n",
    "    return _X[topk.index]\n",
    "\n",
    "def find_similar_metrics_with_sbd(X: pd.DataFrame, col: str, k: int = 5) -> pd.DataFrame:\n",
    "    base_x = X.loc[:, col].to_numpy()\n",
    "    _X: pd.DataFrame = X.loc[:, X.columns!=col]\n",
    "    topk = _X.agg(lambda y: sbd(scipy.stats.zscore(base_x), scipy.stats.zscore(y.to_numpy()))).T.sort_values().head(k)\n",
    "    return _X[topk.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_metrics = build_clustered_metrics(anomalous_cause_service_metrics_df)\n",
    "clustered_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clustered_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import scipy.interpolate\n",
    "\n",
    "# interpolated_anomalous_cause_service_metrics_df = anomalous_cause_service_metrics_df.interpolate(method='spline', order=2, axis=0, limit_direction=\"both\")\n",
    "num_datapoints = anomalous_cause_service_metrics_df.shape[0]\n",
    "plt.rcParams[\"font.size\"] = 9\n",
    "\n",
    "for metrics in clustered_metrics[:3]:\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    axs = fig.subplots(math.ceil(len(metrics)/3), 3)\n",
    "    for ax, metric in zip(axs.flatten(), metrics):\n",
    "        ts = anomalous_cause_service_metrics_df[metric].to_numpy()\n",
    "        zts = scipy.stats.zscore(ts, nan_policy=\"omit\")\n",
    "        ax.plot(range(ts.size), zts)\n",
    "        # ax.plot(scipy.stats.zscore(ts, nan_policy=\"omit\"))\n",
    "        ax.set_title(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_metrics = find_similar_metrics(anomalous_cause_service_metrics_df, \"c-ts-user-mongo_memory_usage_bytes\")\n",
    "similar_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(similar_metrics.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_datapoints = anomalous_cause_service_metrics_df.shape[0]\n",
    "plt.rcParams[\"font.size\"] = 9\n",
    "\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "axs = fig.subplots(math.ceil(len(similar_metrics)/3), 3)\n",
    "for ax, metric in zip(axs.flatten(), similar_metrics.index):\n",
    "    ts = anomalous_cause_service_metrics_df[metric].to_numpy()\n",
    "    zts = scipy.stats.zscore(ts, nan_policy=\"omit\")\n",
    "    ax.plot(range(ts.size), zts)\n",
    "    # ax.plot(scipy.stats.zscore(ts, nan_policy=\"omit\"))\n",
    "    ax.set_title(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from full datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading reduced metrics data\n",
    "\n",
    "import pathlib\n",
    "import pickle\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "NUM_SAMPLES_BY_CHAOS_TYPE: int = 5\n",
    "\n",
    "def load_tsdr():\n",
    "    results = []\n",
    "    parent_path = pathlib.Path(f\"../data/tsdr_rq54b\")\n",
    "    for path in parent_path.iterdir():\n",
    "        with (path / \"record.pkl\").open(\"rb\") as f:\n",
    "            record = pickle.load(f)\n",
    "        with (path / \"reduced_df.pkl\").open(\"rb\") as f:\n",
    "            reduced_df = pickle.load(f)\n",
    "        with (path / \"no_clustering_reduced_df.pkl\").open(\"rb\") as f:\n",
    "            no_clustering_reduced_df = pickle.load(f)\n",
    "        results.append((record, reduced_df, no_clustering_reduced_df))\n",
    "    return results\n",
    "\n",
    "samples_by_chaos_type: dict = defaultdict(list)\n",
    "for record, _, anomalous_metrics_df in load_tsdr():\n",
    "    cause_service: str = record.pk.get_service_by_container(record.chaos_comp())\n",
    "    grouped_metrics_by_service = record.pk.group_metrics_by_service(list(anomalous_metrics_df.columns))\n",
    "    cause_service_metrics: list[str] = grouped_metrics_by_service[cause_service]\n",
    "    anomalous_cause_service_metrics_df = anomalous_metrics_df[cause_service_metrics]\n",
    "    samples_by_chaos_type[record.chaos_type()].append((record, anomalous_cause_service_metrics_df))\n",
    "\n",
    "record_and_anomalous_cause_service_metrics_df = []\n",
    "for samples in samples_by_chaos_type.values():\n",
    "    for sample in random.sample(samples, k=NUM_SAMPLES_BY_CHAOS_TYPE):\n",
    "        record_and_anomalous_cause_service_metrics_df.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TSWindow:\n",
    "    total_records: int\n",
    "    current_record_no: int\n",
    "    record: loader.DatasetRecord\n",
    "    total_metrics_in_current_record: int\n",
    "    current_metrics_no_in_current_record: int\n",
    "    current_metric: str\n",
    "    current_metric_ts: np.ndarray\n",
    "    sli_metric_ts: np.ndarray\n",
    "    sli_metric: str\n",
    "    similar_metrics_df: pd.DataFrame\n",
    "    \n",
    "    def current_pos_info(self) -> str:\n",
    "        return f\"{self.current_record_no}/{self.total_records}:{self.record.chaos_case_full()} -> {self.current_metrics_no_in_current_record}/{self.total_metrics_in_current_record}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_widget_for_clustering(yield_on_click, num_similar_metrics: int = 10) -> widgets.Box:\n",
    "    save_button = widgets.Button(description='Save')\n",
    "    skip_button = widgets.Button(description='Skip')\n",
    "    select_pattern = widgets.Select(\n",
    "        options=[\n",
    "            'Sudden increase', 'Sudden decrease', 'Level shift up', 'Level shift down', \n",
    "            'Steady increase', 'Steady decrease', 'Single spike', 'Single dip',\n",
    "            'Transient level shift up', 'Transient level shift down', 'Multiple spikes', 'Multiple dips', 'Fluctuations',\n",
    "            'White noise', 'Other normal',\n",
    "        ],\n",
    "        rows=15,\n",
    "        layout=widgets.Layout(width='20%'),\n",
    "    )\n",
    "    select_position = widgets.Select(\n",
    "        options=[\"no_anomaly\", \"anomaly_during_fault\", \"anomaly_outside_fault\"],\n",
    "        layout=widgets.Layout(width='15%'),\n",
    "    )\n",
    "    select_similar_metrics = widgets.SelectMultiple(\n",
    "        options=[],\n",
    "        rows=num_similar_metrics+2,\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='40%'),\n",
    "    )\n",
    "    msg_output = widgets.Output(layout={'border': '1px solid black'})\n",
    "    fig_output = widgets.Output(layout={'border': '1px solid black'})\n",
    "\n",
    "    plt.rcParams[\"font.size\"] = 6\n",
    "    plt.rcParams['xtick.labelsize'] = 8\n",
    "    plt.rcParams['ytick.labelsize'] = 8\n",
    "    fig1, cur_and_sli_axs = plt.subplots(1, 2, figsize=(8, 1.5), clear=True)\n",
    "    fig2, similar_axs = plt.subplots(2, num_similar_metrics//2, figsize=(20, 5), clear=True)\n",
    "\n",
    "    def show() -> None:\n",
    "        tsw: TSWindow = next(yield_on_click)\n",
    "        with msg_output:\n",
    "            display(tsw.current_pos_info())\n",
    "\n",
    "        cur_ax, sli_ax = cur_and_sli_axs[0], cur_and_sli_axs[1]\n",
    "        cur_ax.clear()\n",
    "        cur_ax.set_title(tsw.current_metric)\n",
    "        cur_ax.plot(tsw.current_metric_ts)\n",
    "\n",
    "        sli_ax.clear()\n",
    "        sli_ax.plot(tsw.sli_metric_ts)\n",
    "        sli_ax.set_title(f\"SLI\")\n",
    "        for _ax in cur_and_sli_axs:\n",
    "            _ax.axvspan(100, tsw.current_metric_ts.size, color='red', alpha=0.5)\n",
    "        for _ax, metric in zip(similar_axs.flatten(), tsw.similar_metrics_df.columns):\n",
    "            _ax.clear()\n",
    "            _ax.plot(tsw.similar_metrics_df.loc[:, metric])\n",
    "            _ax.set_title(metric)\n",
    "            _ax.axvspan(100, tsw.current_metric_ts.size, color='red', alpha=0.5)\n",
    "        with fig_output:\n",
    "            fig_output.clear_output(wait=True)\n",
    "            display(fig1)\n",
    "            display(fig2)\n",
    "\n",
    "        select_similar_metrics.options = tsw.similar_metrics_df.columns.tolist()\n",
    "\n",
    "    def on_save_click_callback(clicked_button: widgets.Button) -> None:\n",
    "        yield_on_click.send(((select_position.value, select_pattern.value), select_similar_metrics.value))\n",
    "        with msg_output:\n",
    "            msg_output.clear_output(wait=True)\n",
    "            display(f\"Selected {select_pattern.value} and {select_position.value}!\")\n",
    "        show()\n",
    "\n",
    "    save_button.on_click(on_save_click_callback)\n",
    "    plt.close(fig=fig1)\n",
    "    plt.close(fig=fig2)\n",
    "    show()\n",
    "\n",
    "    def on_skip_click_callback(clicked_button: widgets.Button) -> None:\n",
    "        with msg_output:\n",
    "            msg_output.clear_output(wait=True)\n",
    "            display(f\"Skipped\")\n",
    "        show()\n",
    "\n",
    "    skip_button.on_click(on_skip_click_callback)\n",
    "\n",
    "    return widgets.VBox([\n",
    "        msg_output,\n",
    "        fig_output,\n",
    "        widgets.HBox([\n",
    "            select_pattern,\n",
    "            select_position,\n",
    "            select_similar_metrics,\n",
    "            widgets.VBox([save_button, skip_button])\n",
    "        ]),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "MONGODB_EXCLUDE_PATTERN: re.Pattern = re.compile(\n",
    "    r\"^mongodb_.+_([kb|mb|gb|time_ms])$\"\n",
    ")\n",
    "\n",
    "RANGE_VECTOR_DURATION = 60\n",
    "PER_MINUTE_NUM: int = int(RANGE_VECTOR_DURATION / 15) + 1\n",
    "\n",
    "\n",
    "def diff_mongodb_counter_metrics(metric_name: str, ts: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Calculate the difference of mongpdb counter metrics.\n",
    "    MongoDB counter metrics are increasing monotonically, so the difference between two consecutive values is the actual value.\n",
    "    FIXME: This is a temporary solution, and the actual value should be calculated in prometheus fetcher.\n",
    "    \"\"\"\n",
    "    if metric_name.startswith(\"mongodb_\"):\n",
    "        return ts\n",
    "    \n",
    "    rate: np.ndarray\n",
    "    if (\n",
    "        MONGODB_EXCLUDE_PATTERN.match(metric_name) or\n",
    "        np.all(ts == ts[0]) or not np.all(np.diff(ts) >= 0)  # check monotonic increasing\n",
    "    ):\n",
    "        rate = ts\n",
    "    else:\n",
    "        slides = np.lib.stride_tricks.sliding_window_view(ts, PER_MINUTE_NUM)\n",
    "        rate = (np.max(slides, axis=1).reshape(-1) - np.min(slides, axis=1).reshape(-1)) / RANGE_VECTOR_DURATION\n",
    "        for _ in range(PER_MINUTE_NUM - 1):\n",
    "            rate = np.insert(rate, 0, np.NaN)\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "SAVE_DIR = \"../samples/clustering_anomaly_patterns\"\n",
    "\n",
    "def gen_time_series_similar(record_and_anomalous_df: list[tuple[loader.DatasetRecord, pd.DataFrame]]):\n",
    "    now = datetime.datetime.today().strftime('%Y%m%d-%H%M%S')\n",
    "    save_file_name = f\"{SAVE_DIR}/clustering_anomaly_patterns_{now}.jsonl\"\n",
    "\n",
    "    def create_file() -> jsonlines.Writer:\n",
    "        global writer\n",
    "        if os.path.exists(save_file_name):\n",
    "            return writer\n",
    "        writer = jsonlines.open(save_file_name, mode='a', flush=True)  # append mode\n",
    "        return writer\n",
    "\n",
    "    for i, (record, anomalous_df) in enumerate(record_and_anomalous_df):\n",
    "        anomalous_metrics_df = anomalous_df.copy(deep=True).apply(lambda x: pd.Series(diff_mongodb_counter_metrics(x.name, x.to_numpy())), axis=0)\n",
    "        anomalous_metrics_df = anomalous_metrics_df.loc[:, anomalous_metrics_df.apply(lambda x: np.isnan(x).sum() <= 20, axis=0)]\n",
    "\n",
    "        sli_metric: str = \"m-ts-ui-dashboard_nginx_http_response_count_total\"\n",
    "        sli_ts = record.data_df.loc[:, sli_metric].to_numpy()\n",
    "        labeled_metrics_list: list[str] = []\n",
    "        total_num_metrics = anomalous_metrics_df.shape[1]\n",
    "        for metric in anomalous_metrics_df.columns:\n",
    "            if metric in labeled_metrics_list:\n",
    "                continue\n",
    "\n",
    "            ts = anomalous_metrics_df.loc[:, metric].to_numpy()\n",
    "            similar_metrics_df = find_similar_metrics_with_sbd(anomalous_metrics_df, metric, k=10)\n",
    "\n",
    "            tsw = TSWindow(\n",
    "                total_records=len(record_and_anomalous_df),\n",
    "                current_record_no=i+1,\n",
    "                record=record,\n",
    "                total_metrics_in_current_record=total_num_metrics,\n",
    "                current_metric=metric,\n",
    "                current_metric_ts=ts,\n",
    "                current_metrics_no_in_current_record=len(labeled_metrics_list),\n",
    "                sli_metric_ts=sli_ts,\n",
    "                sli_metric=sli_metric,\n",
    "                similar_metrics_df=similar_metrics_df,\n",
    "            )\n",
    "            v = (yield tsw)\n",
    "            if v is None:\n",
    "                continue\n",
    "            ((position, pattern_name), similar_metrics) = v\n",
    "            for _metric in [metric] + list(similar_metrics):\n",
    "                create_file().write({\n",
    "                    'target_app': record.target_app(), \n",
    "                    'chaos_type': record.chaos_type(),\n",
    "                    'chaos_comp': record.chaos_comp(), \n",
    "                    'metric': _metric,\n",
    "                    'anomaly_position': position,\n",
    "                    'anomaly_pattern': pattern_name,\n",
    "                    'time_series': ts.tolist(),\n",
    "                })\n",
    "                labeled_metrics_list.append(_metric)\n",
    "                anomalous_metrics_df.drop(columns=_metric, inplace=True)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f699462e2bf4a57b795c85aeaee9ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(layout=Layout(border_bottom='1px solid black', border_left='1px solid black', border_rigâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "box = create_widget_for_clustering(gen_time_series_similar(record_and_anomalous_cause_service_metrics_df))\n",
    "display(box)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c0f02f5f97634d426ffcfa502db37ef392cddba0a927ded2fc10600c3b8bead5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
