{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FluxInfer RCA method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 10 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from tsdr import tsdr\n",
    "from diagnoser import diag\n",
    "from eval import groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meltria import loader\n",
    "\n",
    "metrics_files = !find /datasets/argowf-chaos-rq54b/ -type f -name \"*.json\" | head -n 3\n",
    "dataset_generator = loader.load_dataset_as_generator(metrics_files, target_metric_types={\n",
    "        \"containers\": True,\n",
    "        \"services\": True,\n",
    "        \"nodes\": True,\n",
    "        \"middlewares\": True,\n",
    "    },\n",
    "    num_datapoints=120,\n",
    ")\n",
    "records = [r for rec in dataset_generator for r in rec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_and_reduced_df: list = []\n",
    "for record in records:\n",
    "    # run tsdr\n",
    "    reducer = tsdr.Tsdr(\"residual_integral\", **{\n",
    "        \"step1_residual_integral_threshold\": 20,\n",
    "        \"step1_residual_integral_change_start_point\": False,\n",
    "        \"step1_residual_integral_change_start_point_n_sigma\": 3,\n",
    "        \"step2_clustering_method_name\": \"dbscan\",\n",
    "        \"step2_dbscan_min_pts\": 2,\n",
    "        \"step2_dbscan_dist_type\": 'sbd',\n",
    "        \"step2_dbscan_algorithm\": 'hdbscan',\n",
    "        \"step2_clustering_series_type\": 'raw',\n",
    "        \"step2_clustering_choice_method\": 'medoid',\n",
    "    })\n",
    "    tsdr_stat, clustering_info, anomaly_points = reducer.run(\n",
    "        X=record.data_df,\n",
    "        pk=record.pk,\n",
    "        max_workers=cpu_count(),\n",
    "    )\n",
    "    reduced_df = tsdr_stat[-1][0]\n",
    "    no_clustering_reduced_df = tsdr_stat[-2][0]\n",
    "    record_and_reduced_df.append((record, reduced_df, no_clustering_reduced_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import diagnoser.metric_node as mn\n",
    "\n",
    "def fisher_z(dm, cm, x, y) -> float:\n",
    "    m = dm.shape[0]\n",
    "    r = cm[x, y]\n",
    "    if 1 - r == 0. or 1 + r == 0.:\n",
    "        r = 1 - 1e-10\n",
    "    zstat = np.sqrt(m - 3) * 0.5 * np.log((1 + r) / (1 - r))\n",
    "    p_val = 2.0 * scipy.stats.norm.sf(np.absolute(zstat))\n",
    "    return p_val\n",
    "\n",
    "def build_wudg(pk, data_df: pd.DataFrame, init_graph_type=\"complete\") -> nx.Graph:\n",
    "    nodes = mn.MetricNodes.from_dataframe(data_df)\n",
    "    g: nx.Graph\n",
    "    match init_graph_type:\n",
    "        case \"complete\":\n",
    "            g = nx.Graph()\n",
    "            for (u, v) in combinations(nodes, 2):\n",
    "                g.add_edge(u, v)\n",
    "        case \"nw_call\":\n",
    "            g = diag.prepare_init_graph(nodes, pk)\n",
    "        case _:\n",
    "            assert False, f\"Unknown init_graph_type: {init_graph_type}\"\n",
    "\n",
    "    dm = data_df.to_numpy()\n",
    "    cm = np.corrcoef(dm.T)\n",
    "    _g = nx.relabel_nodes(g, mapping=nodes.node_to_num)\n",
    "    for (u, v) in _g.edges:\n",
    "        p_val = fisher_z(dm, cm, u, v)\n",
    "        _g[u][v]['weight'] = 1 / p_val if p_val != 0.0 else sys.float_info.max\n",
    "\n",
    "    return nx.relabel_nodes(_g, mapping=nodes.num_to_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record, reduced_df, no_clustering_reduced_df = record_and_reduced_df[1]\n",
    "WUDG = build_wudg(record.pk, reduced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nx_draw(graph: nx.Graph, ax):\n",
    "    pos=nx.spring_layout(graph, weight=None)\n",
    "    nx.draw_networkx(graph, pos=pos, ax=ax, font_size=8, node_size=150)\n",
    "    elabels = nx.get_edge_attributes(graph, 'weight')\n",
    "    for k, weight in elabels.items():\n",
    "        elabels[k] = f\"{weight:.2g}\"\n",
    "    nx.draw_networkx_edge_labels(graph, pos=pos, ax=ax, edge_labels=elabels, font_size=6)\n",
    "\n",
    "def draw_by_graph(graphs: list[nx.Graph], suptitle: str):\n",
    "    fig = plt.figure(1, figsize=(20, 20))\n",
    "    fig.suptitle(suptitle)\n",
    "    axs = fig.subplots(3, 2).flatten()\n",
    "\n",
    "    for ax, g in zip(axs, sorted(graphs, key=lambda g: len(g.nodes), reverse=True)[:5]):\n",
    "        pr = nx.pagerank(g, alpha=0.85) # default\n",
    "        display(sorted(pr.items(), reverse=True, key=lambda x: x[1])[:5])\n",
    "        nx_draw(g, ax)\n",
    "\n",
    "# for suptitle, graphs in ((\"Root contained graph\", root_contained_g), (\"Root uncontained graph\", root_uncontained_g)):\n",
    "#     draw_by_graph(graphs, suptitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = nx.pagerank(WUDG, alpha=0.85) # default\n",
    "display(sorted(pr.items(), reverse=True, key=lambda x: x[1])[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate by AC@k and AVG@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval import groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr: dict[str, float] = nx.pagerank(WUDG, alpha=0.85) # default\n",
    "ranked_metric_to_score: list[tuple[mn.MetricNode, float]] = sorted(pr.items(), reverse=True, key=lambda x: x[1])\n",
    "ranked_metrics = mn.MetricNodes.from_list_of_metric_node([m for m, _ in ranked_metric_to_score])\n",
    "ok, cause_metrics = groundtruth.check_cause_metrics(\n",
    "    record.pk, ranked_metrics, chaos_type=record.chaos_type(), chaos_comp=record.chaos_comp(),\n",
    ")\n",
    "display(ok)\n",
    "for cm in cause_metrics:\n",
    "    display(f\"no:{list(ranked_metrics).index(cm)}\", cm)\n",
    "    plt.plot(reduced_df[str(cm)].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All fault cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meltria import loader\n",
    "\n",
    "metrics_files = !find /datasets/argowf-chaos-rq54b/ -type f -name \"*.json\"\n",
    "dataset_generator = loader.load_dataset_as_generator(metrics_files, target_metric_types={\n",
    "        \"containers\": True,\n",
    "        \"services\": True,\n",
    "        \"nodes\": True,\n",
    "        \"middlewares\": True,\n",
    "    },\n",
    "    num_datapoints=120,\n",
    ")\n",
    "records = [r for rec in dataset_generator for r in rec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_and_reduced_df: list = []\n",
    "for record in records:\n",
    "    # run tsdr\n",
    "    reducer = tsdr.Tsdr(\"residual_integral\", **{\n",
    "        \"step1_residual_integral_threshold\": 20,\n",
    "        \"step1_residual_integral_change_start_point\": False,\n",
    "        \"step1_residual_integral_change_start_point_n_sigma\": 3,\n",
    "        \"step2_clustering_method_name\": \"dbscan\",\n",
    "        \"step2_dbscan_min_pts\": 2,\n",
    "        \"step2_dbscan_dist_type\": 'sbd',\n",
    "        \"step2_dbscan_algorithm\": 'hdbscan',\n",
    "        \"step2_clustering_series_type\": 'raw',\n",
    "        \"step2_clustering_choice_method\": 'medoid',\n",
    "    })\n",
    "    tsdr_stat, clustering_info, anomaly_points = reducer.run(\n",
    "        X=record.data_df,\n",
    "        pk=record.pk,\n",
    "        max_workers=cpu_count(),\n",
    "    )\n",
    "    reduced_df = tsdr_stat[-1][0]\n",
    "    no_clustering_reduced_df = tsdr_stat[-2][0]\n",
    "    record_and_reduced_df.append((record, reduced_df, no_clustering_reduced_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/tmp/ipykernel_354894/717067231.py:31: RuntimeWarning: overflow encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "# parallelize\n",
    "import joblib\n",
    "\n",
    "wudgs: list[tuple[nx.Graph, loader.DatasetRecord, pd.DataFrame, pd.DataFrame]]\n",
    "wudgs = joblib.Parallel(n_jobs=-1)(joblib.delayed(build_wudg)(record.pk, reduced_df) for record, reduced_df, no_clustering_reduced_df in record_and_reduced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/src/github.com/ai4sre/meltria-analyzer/.venv/lib/python3.10/site-packages/scipy/sparse/_compressed.py:646: RuntimeWarning: overflow encountered in reduceat\n",
      "  value = ufunc.reduceat(data,\n",
      "/home/ubuntu/src/github.com/ai4sre/meltria-analyzer/.venv/lib/python3.10/site-packages/scipy/sparse/_compressed.py:646: RuntimeWarning: overflow encountered in reduceat\n",
      "  value = ufunc.reduceat(data,\n",
      "/home/ubuntu/src/github.com/ai4sre/meltria-analyzer/.venv/lib/python3.10/site-packages/scipy/sparse/_compressed.py:646: RuntimeWarning: overflow encountered in reduceat\n",
      "  value = ufunc.reduceat(data,\n",
      "/home/ubuntu/src/github.com/ai4sre/meltria-analyzer/.venv/lib/python3.10/site-packages/scipy/sparse/_compressed.py:646: RuntimeWarning: overflow encountered in reduceat\n",
      "  value = ufunc.reduceat(data,\n",
      "/home/ubuntu/src/github.com/ai4sre/meltria-analyzer/.venv/lib/python3.10/site-packages/scipy/sparse/_compressed.py:646: RuntimeWarning: overflow encountered in reduceat\n",
      "  value = ufunc.reduceat(data,\n",
      "/home/ubuntu/src/github.com/ai4sre/meltria-analyzer/.venv/lib/python3.10/site-packages/scipy/sparse/_compressed.py:646: RuntimeWarning: overflow encountered in reduceat\n",
      "  value = ufunc.reduceat(data,\n",
      "/home/ubuntu/src/github.com/ai4sre/meltria-analyzer/.venv/lib/python3.10/site-packages/scipy/sparse/_compressed.py:646: RuntimeWarning: overflow encountered in reduceat\n",
      "  value = ufunc.reduceat(data,\n",
      "/home/ubuntu/src/github.com/ai4sre/meltria-analyzer/.venv/lib/python3.10/site-packages/scipy/sparse/_compressed.py:646: RuntimeWarning: overflow encountered in reduceat\n",
      "  value = ufunc.reduceat(data,\n",
      "/home/ubuntu/src/github.com/ai4sre/meltria-analyzer/.venv/lib/python3.10/site-packages/scipy/sparse/_compressed.py:646: RuntimeWarning: overflow encountered in reduceat\n",
      "  value = ufunc.reduceat(data,\n",
      "/home/ubuntu/src/github.com/ai4sre/meltria-analyzer/.venv/lib/python3.10/site-packages/scipy/sparse/_compressed.py:646: RuntimeWarning: overflow encountered in reduceat\n",
      "  value = ufunc.reduceat(data,\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "prs: list[tuple[dict, loader.DatasetRecord, pd.DataFrame, pd.DataFrame]] = []\n",
    "prs = joblib.Parallel(n_jobs=-1)(joblib.delayed(nx.pagerank)(wudg, alpha=0.85) for wudg in wudgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval import validation\n",
    "\n",
    "def check_validate_record(record) -> bool:\n",
    "    return validation.check_valid_dataset(\n",
    "        record, labbeling={\"n_sigma_rule\": {\"n_sigmas\": [2, 3]}}, fault_inject_time_index=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 77\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'eval.groundtruth' has no attribute 'check_valid_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_354894/2451112601.py\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_and_reduced_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduced_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_clustering_reduced_df\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_and_reduced_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_validate_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"skipping {record.chaos_case_full()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_354894/1842128292.py\u001b[0m in \u001b[0;36mcheck_validate_record\u001b[0;34m(record)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_validate_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     return groundtruth.check_valid_dataset(\n\u001b[0m\u001b[1;32m      3\u001b[0m         record, labbeling={\"n_sigma_rule\": {\"n_sigmas\": [2, 3]}}, fault_inject_time_index=99)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'eval.groundtruth' has no attribute 'check_valid_dataset'"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "anomaly_case_sizes = len(prs)\n",
    "top_k_set = range(1, 11)\n",
    "ac_k: dict[int, float] = {k: 0.0 for k in top_k_set}\n",
    "rank_by_case: dict[str, list[int]] = defaultdict(list)\n",
    "print(len(prs), len(record_and_reduced_df))\n",
    "for pr, (record, reduced_df, non_clustering_reduced_df) in zip(prs, record_and_reduced_df):\n",
    "    if not check_validate_record(record):\n",
    "        print(f\"skipping {record.chaos_case_full()}\")\n",
    "        continue\n",
    "    ranked_metric_to_score: list[tuple[mn.MetricNode, float]] = sorted(pr.items(), reverse=True, key=lambda x: x[1])\n",
    "    ranked_metrics = mn.MetricNodes.from_list_of_metric_node([m for m, _ in ranked_metric_to_score])\n",
    "    _, cause_metrics = groundtruth.check_cause_metrics(\n",
    "        record.pk, ranked_metrics, chaos_type=record.chaos_type(), chaos_comp=record.chaos_comp(),\n",
    "    )\n",
    "    if len(cause_metrics) == 0:\n",
    "        print(f\"no cause metrics: {record.chaos_case_full()}\")\n",
    "        continue\n",
    "    rank: int = sorted([list(ranked_metrics).index(cm) for cm in cause_metrics])[0] + 1\n",
    "    print(f\"rank: {rank}, {record.chaos_case_full()}\")\n",
    "    rank_by_case[record.chaos_type()].append(rank)\n",
    "    # plt.plot(reduced_df[str(cm)].to_numpy())\n",
    "\n",
    "for k in top_k_set:\n",
    "    ac_k[k] = sum([1 if rank <= k else 0 for rank in chain.from_iterable(rank_by_case.values())]) / anomaly_case_sizes\n",
    "display(\"AC@K\", ac_k)\n",
    "\n",
    "avg_k = {}\n",
    "for k in top_k_set:\n",
    "    avg_k[k] = sum([ac_k[j] for j in range(1, k+1)]) / k\n",
    "display(\"AVG@k\", avg_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with service granulally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "anomaly_case_sizes = len(prs)\n",
    "top_k_set = range(1, 11)\n",
    "ac_k: dict[int, float] = {k: 0.0 for k in top_k_set}\n",
    "rank_by_case: dict[str, list[int]] = defaultdict(list)\n",
    "print(len(prs), len(record_and_reduced_df))\n",
    "for pr, (record, reduced_df, non_clustering_reduced_df) in zip(prs, record_and_reduced_df):\n",
    "    if not check_validate_record(record):\n",
    "        print(f\"skipping {record.chaos_case_full()}\")\n",
    "        continue\n",
    "    chaos_service: str = record.chaos_comp().removesuffix(\"-service\").removesuffix(\"-mongo\")\n",
    "    ranked_metric_to_score: list[tuple[mn.MetricNode, float]] = sorted(pr.items(), reverse=True, key=lambda x: x[1])\n",
    "    rank: int = sorted([i+1 for i, (m, _) in enumerate(ranked_metric_to_score) if m.comp.startswith(chaos_service)])[0]\n",
    "    print(f\"rank: {rank}, {record.chaos_case_full()}\")\n",
    "    rank_by_case[record.chaos_type()].append(rank)\n",
    "    # plt.plot(reduced_df[str(cm)].to_numpy())\n",
    "\n",
    "for k in top_k_set:\n",
    "    ac_k[k] = sum([1 if rank <= k else 0 for rank in chain.from_iterable(rank_by_case.values())]) / anomaly_case_sizes\n",
    "display(\"AC@K\", ac_k)\n",
    "\n",
    "avg_k = {}\n",
    "for k in top_k_set:\n",
    "    avg_k[k] = sum([ac_k[j] for j in range(1, k+1)]) / k\n",
    "display(\"AVG@k\", avg_k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c0f02f5f97634d426ffcfa502db37ef392cddba0a927ded2fc10600c3b8bead5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
