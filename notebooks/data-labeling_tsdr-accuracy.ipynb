{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data labeling tool for evaluating tsdr accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"font.family\"] = \"DejaVu Sans\"\n",
    "plt.rcParams[\"font.size\"] = 7\n",
    "plt.rcParams['xtick.labelsize'] = 9\n",
    "plt.rcParams['ytick.labelsize'] = 9\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'\n",
    "plt.rcParams['axes.linewidth'] = 1.0\n",
    "plt.rcParams['axes.grid'] = True\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from tsdr import tsdr\n",
    "from eval import groundtruth\n",
    "from meltria import loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading reduced metrics data\n",
    "\n",
    "import pathlib\n",
    "import pickle\n",
    "\n",
    "DATASET_ID = \"rq54b\"\n",
    "\n",
    "def load_tsdr():\n",
    "    results = []\n",
    "    parent_path = pathlib.Path(f\"../data/tsdr_{DATASET_ID}\")\n",
    "    for path in parent_path.iterdir():\n",
    "        with (path / \"record.pkl\").open(\"rb\") as f:\n",
    "            record = pickle.load(f)\n",
    "        with (path / \"reduced_df.pkl\").open(\"rb\") as f:\n",
    "            reduced_df = pickle.load(f)\n",
    "        with (path / \"no_clustering_reduced_df.pkl\").open(\"rb\") as f:\n",
    "            no_clustering_reduced_df = pickle.load(f)\n",
    "        results.append((record, reduced_df, no_clustering_reduced_df))\n",
    "    return results\n",
    "\n",
    "datasets = load_tsdr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_injected_fault_dataset_entries_texts: list[str] = \"\"\"\n",
    "ts-food-service/pod-memory-hog/0\n",
    "ts-travel2-service/pod-memory-hog/0\n",
    "ts-consign-mongo/pod-cpu-hog/0\n",
    "ts-travel2-mongo/pod-network-loss/0\n",
    "ts-train-mongo/pod-network-loss/0\n",
    "ts-station-service/pod-cpu-hog/0\n",
    "ts-auth-mongo/pod-memory-hog/0\n",
    "ts-user-service/pod-network-loss/0\n",
    "ts-travel-service/pod-memory-hog/0\n",
    "ts-order-other-service/pod-network-loss/0\n",
    "ts-food-mongo/pod-cpu-hog/0\n",
    "ts-train-service/pod-network-loss/0\n",
    "ts-price-service/pod-network-loss/0\n",
    "ts-order-service/pod-cpu-hog/0\n",
    "ts-auth-mongo/pod-cpu-hog/0\n",
    "ts-train-service/pod-cpu-hog/0\n",
    "ts-auth-service/pod-memory-hog/0\n",
    "ts-order-service/pod-network-loss/0\n",
    "ts-travel-mongo/pod-network-loss/0\n",
    "ts-basic-service/pod-memory-hog/0\n",
    "ts-station-service/pod-network-loss/0\n",
    "ts-basic-service/pod-cpu-hog/0\n",
    "ts-order-mongo/pod-memory-hog/0\n",
    "ts-food-mongo/pod-network-loss/0\n",
    "ts-user-service/pod-memory-hog/0\n",
    "ts-order-mongo/pod-cpu-hog/0\n",
    "ts-travel2-service/pod-network-loss/0\n",
    "ts-station-mongo/pod-cpu-hog/0\n",
    "ts-auth-mongo/pod-network-loss/0\n",
    "ts-food-mongo/pod-memory-hog/0\n",
    "ts-price-mongo/pod-network-loss/0\n",
    "ts-basic-service/pod-network-loss/0\n",
    "ts-order-service/pod-memory-hog/0\n",
    "ts-food-service/pod-cpu-hog/0\n",
    "ts-auth-service/pod-network-loss/0\n",
    "ts-station-mongo/pod-network-loss/0\n",
    "ts-train-service/pod-memory-hog/0\n",
    "ts-travel-service/pod-network-loss/0\n",
    "ts-order-mongo/pod-network-loss/0\n",
    "ts-order-other-service/pod-memory-hog/0\n",
    "ts-consign-mongo/pod-memory-hog/0\n",
    "ts-train-mongo/pod-memory-hog/0\n",
    "ts-travel-service/pod-cpu-hog/0\n",
    "ts-preserve-service/pod-memory-hog/0\n",
    "ts-auth-service/pod-cpu-hog/0\n",
    "ts-station-service/pod-memory-hog/0\n",
    "\"\"\".splitlines()\n",
    "well_injected_fault_dataset_entries: list[tuple[str, str]] = [\n",
    "    tuple(line.rstrip(\"/0\").split(\"/\")) for line in well_injected_fault_dataset_entries_texts\n",
    "][1:]\n",
    "\n",
    "well_injected_fault_datasets = [(record, reduced_df, no_clustering_reduced_df) for record, reduced_df, no_clustering_reduced_df in datasets if (record.chaos_comp(), record.chaos_type()) in well_injected_fault_dataset_entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# NUM_SAMPLES_BY_CHAOS_TYPE = 5\n",
    "\n",
    "samples_by_chaos_type: dict = defaultdict(list)\n",
    "for record, _, _ in random.sample(well_injected_fault_datasets, k=len(well_injected_fault_datasets)):\n",
    "    filtered_df: pd.DataFrame = tsdr.filter_out_no_change_metrics(record.data_df, parallel=True)\n",
    "    gt_candidates = groundtruth.select_ground_truth_metrics_in_routes(\n",
    "        record.pk, filtered_df.columns.to_list(), record.chaos_type(), record.chaos_comp(), gt_opts={\n",
    "            \"cause_middleware\": True,\n",
    "            \"cause_service\": True,\n",
    "            \"neighbors_in_cause_service\": True,\n",
    "            \"propagated_route\": True,\n",
    "        }\n",
    "    )\n",
    "    # flatten and unique to remove duplicates\n",
    "    gt_metrics = list(set([metric for metrics, _ in gt_candidates for metric in metrics]))\n",
    "\n",
    "    samples_by_chaos_type[record.chaos_type()].append((record, filtered_df[gt_metrics]))\n",
    "\n",
    "record_and_faulty_metrics_df = []\n",
    "for chaos_type, samples in samples_by_chaos_type.items():\n",
    "    record_and_faulty_metrics_df.extend(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "RANGE_VECTOR_DURATION = 60\n",
    "PER_MINUTE_NUM: int = int(RANGE_VECTOR_DURATION / 15) + 1\n",
    "\n",
    "JVM_TOMCAT_PATTERN: re.Pattern = re.compile(\n",
    "    r\"^Tomcat_.+_(requestCount|maxTime|processingTime|requestProcessingTime|errorCount|[b|B]ytesSent|[b|B]ytesReceived)$\"\n",
    ")\n",
    "JVM_OS_PATTERN: re.Pattern = re.compile(r\"^java_lang_OperatingSystem_.+_ProcessCpuTime$\")\n",
    "JVM_JAVA_PATTERN: re.Pattern = re.compile(r\"^java_lang_.+[t|T]ime$\")\n",
    "\n",
    "MONGODB_EXCLUDE_PATTERN: re.Pattern = re.compile(\n",
    "    r\"^mongodb_.+_([kb|mb|gb|time_ms])$\"\n",
    ")\n",
    "\n",
    "\n",
    "def rate_of_metrics(ts: np.ndarray) -> np.ndarray:\n",
    "    slides = np.lib.stride_tricks.sliding_window_view(ts, PER_MINUTE_NUM)\n",
    "    rate = (np.max(slides, axis=1).reshape(-1) - np.min(slides, axis=1).reshape(-1)) / RANGE_VECTOR_DURATION\n",
    "    first_val = rate[0]\n",
    "    for _ in range(PER_MINUTE_NUM - 1):\n",
    "        rate = np.insert(rate, 0, first_val)  # backfill\n",
    "    return rate\n",
    "\n",
    "\n",
    "def should_not_rate_metrics(x: np.ndarray) -> bool:\n",
    "    return bool(\n",
    "        np.all(x == x[0])             # check all values are the same\n",
    "        or np.any(np.diff(x) < 0)     # check not monotonic increasing\n",
    "        or np.any(x != np.round(x))  # check including float because a counter metric should be integer.\n",
    "    )\n",
    "\n",
    "\n",
    "def rate_of_metrics_with_check(metric: str, ts: np.ndarray) -> np.ndarray:\n",
    "    if not metric.startswith(\"m-\"):\n",
    "        return ts\n",
    "\n",
    "    metric_comp, metric_base_name = metric.split(\"-\", maxsplit=1)[1].split(\"_\", maxsplit=1)\n",
    "\n",
    "    if should_not_rate_metrics(ts):\n",
    "        return ts\n",
    "\n",
    "    if MONGODB_EXCLUDE_PATTERN.match(metric_base_name):\n",
    "        return ts\n",
    "\n",
    "    if (\n",
    "        JVM_JAVA_PATTERN.match(metric_base_name)\n",
    "        or JVM_OS_PATTERN.match(metric_base_name)\n",
    "        or JVM_TOMCAT_PATTERN.match(metric_base_name)\n",
    "    ):\n",
    "        # work around rate_of_metrics(ts)\n",
    "        return ts\n",
    "\n",
    "    return rate_of_metrics(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tslearn.neighbors import KNeighborsTimeSeries\n",
    "from tslearn.metrics import dtw\n",
    "\n",
    "from tsdr.clustering.sbd import sbd\n",
    "\n",
    "def build_clustered_metrics(X: pd.DataFrame, k: int = 5):\n",
    "    # remove metrics that have nan.\n",
    "    # _ts = ts.apply(lambda x: pd.Series(interp1d(x.to_numpy())), axis=0)\n",
    "    _X = X.loc[:, X.apply(lambda x: not x.isna().any())]\n",
    "    # def distance(x, y) -> float:\n",
    "    #     return fastdtw(x, y)[0]\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, metric=dtw).fit(_X.values.T)\n",
    "    idx_arrays = nbrs.kneighbors(return_distance=False)\n",
    "    return [_X.columns[idx].to_list() for idx in idx_arrays]\n",
    "\n",
    "def find_similar_metrics(X: pd.DataFrame, col: str, k: int = 5) -> pd.DataFrame:\n",
    "    base_x = X.loc[:, col].to_numpy()\n",
    "    _X: pd.DataFrame = X.loc[:, X.columns!=col]\n",
    "    topk = _X.agg(lambda y: dtw(base_x, y.to_numpy())).T.sort_values().head(k)\n",
    "    return _X[topk.index]\n",
    "\n",
    "def find_similar_metrics_with_sbd(X: pd.DataFrame, col: str, k: int = 5) -> tuple[pd.DataFrame, pd.Series]:\n",
    "    base_x = X.loc[:, col].to_numpy()\n",
    "    _X: pd.DataFrame = X.loc[:, X.columns!=col]\n",
    "    topk = _X.agg(lambda y: sbd(scipy.stats.zscore(base_x, nan_policy=\"omit\"), scipy.stats.zscore(y.to_numpy(), nan_policy=\"omit\"))).T.sort_values().head(k)\n",
    "    return _X[topk.index], topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TSWindow:\n",
    "    total_records: int\n",
    "    current_record_no: int\n",
    "    record: loader.DatasetRecord\n",
    "    total_metrics_in_current_record: int\n",
    "    current_metrics_no_in_current_record: int\n",
    "    current_metric: str\n",
    "    current_metric_ts: np.ndarray\n",
    "    sli_metric_ts: np.ndarray\n",
    "    sli_metric: str\n",
    "    similar_metrics_df: pd.DataFrame\n",
    "    similar_metrics_top_score: pd.Series\n",
    "    \n",
    "    def current_pos_info(self) -> str:\n",
    "        return f\"{self.current_record_no}/{self.total_records}:{self.record.chaos_case_full()} -> {self.current_metrics_no_in_current_record}/{self.total_metrics_in_current_record}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%gui asyncio\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "def create_widget_for_clustering(yield_on_click, num_similar_metrics: int = 10) -> widgets.Box:\n",
    "    save_button = widgets.Button(description='Save')\n",
    "    skip_button = widgets.Button(description='Skip')\n",
    "    skip_record_button = widgets.Button(description='Skip Record')\n",
    "    select_pattern = widgets.Select(\n",
    "        options=[\n",
    "            'Sudden increase', 'Sudden decrease', 'Level shift up', 'Level shift down', \n",
    "            'Steady increase', 'Steady decrease', 'Single spike', 'Single dip',\n",
    "            'Transient level shift up', 'Transient level shift down', 'Multiple spikes', 'Multiple dips', 'Fluctuations',\n",
    "            'White noise', 'Other normal',\n",
    "        ],\n",
    "        rows=15,\n",
    "        layout=widgets.Layout(width='20%'),\n",
    "    )\n",
    "    select_position = widgets.Select(\n",
    "        options=[\"no_anomaly\", \"anomaly_during_fault\", \"anomaly_outside_fault\"],\n",
    "        layout=widgets.Layout(width='15%'),\n",
    "    )\n",
    "    select_similar_metrics = widgets.SelectMultiple(\n",
    "        options=[],\n",
    "        rows=num_similar_metrics+2,\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='40%'),\n",
    "    )\n",
    "    msg_output = widgets.Output(layout={'border': '1px solid black'})\n",
    "    fig_output = widgets.Output(layout={'border': '1px solid black'})\n",
    "    log_output = widgets.Output(layout={'border': '1px solid black'})\n",
    "\n",
    "    plt.rcParams[\"font.size\"] = 6\n",
    "    plt.rcParams['xtick.labelsize'] = 8\n",
    "    plt.rcParams['ytick.labelsize'] = 8\n",
    "    fig1, cur_and_sli_axs = plt.subplots(1, 2, figsize=(8, 1.5), clear=True)\n",
    "    fig2, similar_axs = plt.subplots(2, num_similar_metrics//2, figsize=(20, 5), clear=True)\n",
    "\n",
    "    def show(tsw: TSWindow) -> None:\n",
    "        with msg_output:\n",
    "            msg_output.clear_output(wait=True)\n",
    "            display(tsw.current_pos_info())\n",
    "\n",
    "        cur_ax, sli_ax = cur_and_sli_axs[0], cur_and_sli_axs[1]\n",
    "        cur_ax.clear()\n",
    "        cur_ax.set_title(tsw.current_metric)\n",
    "        cur_ax.plot(tsw.current_metric_ts)\n",
    "\n",
    "        sli_ax.clear()\n",
    "        sli_ax.plot(tsw.sli_metric_ts)\n",
    "        sli_ax.set_title(f\"SLI\")\n",
    "        for _ax in cur_and_sli_axs:\n",
    "            _ax.axvspan(100, tsw.current_metric_ts.size, color='red', alpha=0.5)\n",
    "\n",
    "        for _ax in similar_axs.flatten():\n",
    "            _ax.clear()\n",
    "        for i, (_ax, metric) in enumerate(zip(similar_axs.flatten(), tsw.similar_metrics_df.columns)):\n",
    "            _ax.plot(tsw.similar_metrics_df.loc[:, metric])\n",
    "            _ax.set_title(f\"{i+1}: {metric}\")\n",
    "            _ax.axvspan(100, tsw.current_metric_ts.size, color='red', alpha=0.5)\n",
    "\n",
    "        with fig_output:\n",
    "            fig_output.clear_output(wait=True)\n",
    "            display(fig1)\n",
    "            display(fig2)\n",
    "        \n",
    "        with log_output:\n",
    "            log_output.clear_output(wait=True)\n",
    "            display(tsw.similar_metrics_top_score)\n",
    "\n",
    "        select_similar_metrics.options = tsw.similar_metrics_df.columns.tolist()\n",
    "\n",
    "    def on_save_click_callback(clicked_button: widgets.Button) -> None:\n",
    "        tsw: TSWindow = yield_on_click.send(((select_position.value, select_pattern.value), select_similar_metrics.value))\n",
    "        with msg_output:\n",
    "            msg_output.clear_output(wait=True)\n",
    "            display(f\"Selected {select_pattern.value} and {select_position.value}!\")\n",
    "        show(tsw)\n",
    "\n",
    "    save_button.on_click(on_save_click_callback)\n",
    "    plt.close(fig=fig1)\n",
    "    plt.close(fig=fig2)\n",
    "    show(next(yield_on_click))\n",
    "\n",
    "    def on_skip_click_callback(clicked_button: widgets.Button) -> None:\n",
    "        tsw: TSWindow = yield_on_click.send((\"skip\"))\n",
    "        with msg_output:\n",
    "            msg_output.clear_output(wait=True)\n",
    "            display(f\"Skipped\")\n",
    "        show(tsw)\n",
    "\n",
    "    skip_button.on_click(on_skip_click_callback)\n",
    "\n",
    "    def on_skip_record_click_callback(clicked_button: widgets.Button) -> None:\n",
    "        tsw: TSWindow = yield_on_click.send((\"skip_record\"))\n",
    "        with msg_output:\n",
    "            msg_output.clear_output(wait=True)\n",
    "            display(f\"Record skipped\")\n",
    "        show(tsw)\n",
    "    \n",
    "    skip_record_button.on_click(on_skip_record_click_callback)\n",
    "\n",
    "    return widgets.VBox([\n",
    "        msg_output,\n",
    "        fig_output,\n",
    "        widgets.HBox([\n",
    "            select_pattern,\n",
    "            select_position,\n",
    "            select_similar_metrics,\n",
    "            widgets.VBox([save_button, skip_button, skip_record_button])\n",
    "        ]),\n",
    "        log_output,\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import jsonlines\n",
    "\n",
    "SAVE_DIR = \"../samples/tsdr_anomaly_patterns\"\n",
    "\n",
    "def gen_time_series_similar(\n",
    "    record_and_faulty_df: list[tuple[loader.DatasetRecord, pd.DataFrame]],\n",
    "    time: str = datetime.datetime.today().strftime('%Y%m%d-%H%M%S'),\n",
    "):\n",
    "    save_file_name = f\"{SAVE_DIR}/tsdr_anomaly_patterns_{time}.jsonl\"\n",
    "\n",
    "    for i, (record, _faulty_df) in enumerate(record_and_faulty_df):\n",
    "        faulty_metrics_df = _faulty_df.copy(deep=True).apply(\n",
    "            lambda x: pd.Series(rate_of_metrics_with_check(x.name, x.to_numpy())\n",
    "        ), axis=0)\n",
    "        faulty_metrics_df.interpolate(method=\"bfill\", inplace=True)\n",
    "        faulty_metrics_df.interpolate(method=\"ffill\", inplace=True)\n",
    "        faulty_metrics_df = faulty_metrics_df.loc[:, faulty_metrics_df.apply(lambda x: np.isnan(x).sum() <= 20, axis=0)]\n",
    "\n",
    "        sli_metric: str = \"m-ts-ui-dashboard_nginx_http_response_count_total\"\n",
    "        sli_ts = record.data_df.loc[:, sli_metric].to_numpy()\n",
    "        labeled_metrics: set[str] = set()\n",
    "        total_num_metrics = faulty_metrics_df.shape[1]\n",
    "        faulty_metrics = faulty_metrics_df.columns.tolist()\n",
    "        for current_metric in faulty_metrics:\n",
    "            if current_metric in labeled_metrics:\n",
    "                continue\n",
    "\n",
    "            similar_metrics_df, top_score = find_similar_metrics_with_sbd(\n",
    "                faulty_metrics_df, current_metric, k=10,\n",
    "            )\n",
    "\n",
    "            tsw = TSWindow(\n",
    "                total_records=len(record_and_faulty_df),\n",
    "                current_record_no=i+1,\n",
    "                record=record,\n",
    "                total_metrics_in_current_record=total_num_metrics,\n",
    "                current_metric=current_metric,\n",
    "                current_metric_ts=faulty_metrics_df.loc[:, current_metric].to_numpy(),\n",
    "                current_metrics_no_in_current_record=len(labeled_metrics),\n",
    "                sli_metric_ts=sli_ts,\n",
    "                sli_metric=sli_metric,\n",
    "                similar_metrics_df=similar_metrics_df,\n",
    "                similar_metrics_top_score=top_score,\n",
    "            )\n",
    "\n",
    "            # Sent tsw to the UI widget's callback\n",
    "            (v) = (yield tsw)\n",
    "            if v == \"skip\":  # skip button is clicked\n",
    "                continue\n",
    "            elif v == \"skip_record\":  # if skip_record button is clicked\n",
    "                break\n",
    "            ((position, pattern_name), similar_metrics) = v  # if save button is clicked\n",
    "\n",
    "            with jsonlines.open(save_file_name, mode='a', flush=True) as writer:\n",
    "                _metrics: list[str] = [current_metric] + list(similar_metrics)\n",
    "                for _metric in _metrics:\n",
    "                    writer.write({\n",
    "                        'dataset_id': DATASET_ID, \n",
    "                        'target_app': record.target_app(), \n",
    "                        'chaos_type': record.chaos_type(),\n",
    "                        'chaos_comp': record.chaos_comp(), \n",
    "                        'metric': _metric,\n",
    "                        'anomaly_position': position,\n",
    "                        'anomaly_pattern': pattern_name,\n",
    "                        'time_series': faulty_metrics_df.loc[:, _metric].to_numpy().tolist(),\n",
    "                    })\n",
    "                    labeled_metrics.add(_metric)\n",
    "                faulty_metrics_df.drop(columns=_metrics, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f72905c5987441eb58b46c8e06fa9bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(layout=Layout(border_bottom='1px solid black', border_left='1px solid black', border_rig…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_553656/1805070859.py\u001b[0m in \u001b[0;36mon_skip_click_callback\u001b[0;34m(clicked_button)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_skip_click_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclicked_button\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwidgets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mButton\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mtsw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTSWindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myield_on_click\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmsg_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mmsg_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_553656/2472089786.py\u001b[0m in \u001b[0;36mgen_time_series_similar\u001b[0;34m(record_and_faulty_df, time)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"skip_record\"\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# if skip_record button is clicked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilar_metrics\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m  \u001b[0;31m# if save button is clicked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mjsonlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "box = create_widget_for_clustering(\n",
    "    gen_time_series_similar(record_and_faulty_metrics_df)\n",
    ")\n",
    "display(box)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c0f02f5f97634d426ffcfa502db37ef392cddba0a927ded2fc10600c3b8bead5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
