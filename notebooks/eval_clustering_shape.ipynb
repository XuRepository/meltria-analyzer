{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of clustering (shape-based)\n",
    "\n",
    "- Use labeled data collected from anomaly_patterns_clustering_shape.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import glob\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../samples/clustering_anomaly_patterns/clustering_anomaly_patterns_20221030-162851.jsonl'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Level shift down': 982,\n",
       "             'Single spike': 458,\n",
       "             'Level shift up': 550,\n",
       "             'Multiple spikes': 321,\n",
       "             'Single dip': 376,\n",
       "             'Fluctuations': 86,\n",
       "             'Other normal': 364,\n",
       "             'Steady increase': 711,\n",
       "             'Transient level shift up': 51,\n",
       "             'Transient level shift down': 17,\n",
       "             'White noise': 112,\n",
       "             'Steady decrease': 52,\n",
       "             'Sudden increase': 67,\n",
       "             'Multiple dips': 12,\n",
       "             'Sudden decrease': 49})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'anomaly_during_fault': 3045,\n",
       "             'anomaly_outside_fault': 695,\n",
       "             'no_anomaly': 468})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'../samples/clustering_anomaly_patterns/clustering_anomaly_patterns_20221028-172414.jsonl'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Level shift up': 34,\n",
       "             'Single dip': 49,\n",
       "             'Single spike': 65,\n",
       "             'Other normal': 12,\n",
       "             'Multiple spikes': 34,\n",
       "             'Steady increase': 8,\n",
       "             'Transient level shift down': 4,\n",
       "             'Transient level shift up': 10,\n",
       "             'Level shift down': 10,\n",
       "             'White noise': 8,\n",
       "             'Multiple dips': 2,\n",
       "             'Sudden increase': 2,\n",
       "             'Fluctuations': 3,\n",
       "             'Steady decrease': 3})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'anomaly_during_fault': 149,\n",
       "             'anomaly_outside_fault': 75,\n",
       "             'no_anomaly': 20})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for f in glob.glob(\"../samples/clustering_anomaly_patterns/*.jsonl\"):\n",
    "    with jsonlines.open(f) as reader:\n",
    "        aggr = defaultdict(int)\n",
    "        aggr2 = defaultdict(int)\n",
    "        for obj in reader:\n",
    "            aggr[obj[\"anomaly_pattern\"]] += 1\n",
    "            aggr2[obj[\"anomaly_position\"]] += 1\n",
    "    if len(aggr) < 1:\n",
    "        continue\n",
    "    display(f, aggr, aggr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy.interpolate\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from tsdr import tsdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = \"../samples/clustering_anomaly_patterns/clustering_anomaly_patterns_20221030-162851.jsonl\"\n",
    "\n",
    "samples: dict = {}\n",
    "time_series_by_case: dict[tuple[str, str], list[tuple[str, np.ndarray]]] = defaultdict(list)\n",
    "with jsonlines.open(fpath) as reader:\n",
    "    for obj in reader:\n",
    "        time_series_by_case[(obj[\"chaos_type\"], obj[\"chaos_comp\"])].append((obj[\"metric\"], np.array(obj[\"time_series\"])))\n",
    "        \n",
    "        key = (obj[\"chaos_type\"], obj[\"chaos_comp\"], obj[\"metric\"])\n",
    "        samples[key] = {\"series\": np.array(obj[\"time_series\"], dtype=np.float64)}\n",
    "        apos, apattern = obj[\"anomaly_position\"], obj[\"anomaly_pattern\"]\n",
    "        if apos == \"no_anomaly\" or apattern in [\"White noise\", \"Other normal\"]:\n",
    "            samples[key].update({\n",
    "                \"anomaly_type\": \"type0\",\n",
    "                \"anomaly_pattern\": \"normal\",\n",
    "                \"anomaly_position\": apos,\n",
    "            })\n",
    "        else:\n",
    "            match apattern:\n",
    "                # Type 1\n",
    "                case \"Level shift down\" | \"Level shift up\" | \"Steady decrease\" | \"Steady increase\" | \"Sudden decrease\" | \"Sudden increase\":\n",
    "                    samples[key].update({\n",
    "                        \"anomaly_type\": \"type1\",\n",
    "                        \"anomaly_pattern\": apattern,\n",
    "                        \"anomaly_position\": apos,\n",
    "                    })\n",
    "                # Type 2\n",
    "                case \"Fluctuations\" | \"Multiple dips\" | \"Multiple spikes\" | \"Single dip\" | \"Single spike\" | \"Transient level shift down\" | \"Transient level shift up\":\n",
    "                    samples[key].update({\n",
    "                        \"anomaly_type\": \"type2\",\n",
    "                        \"anomaly_pattern\": apattern,\n",
    "                        \"anomaly_position\": apos,\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "from meltria.priorknowledge import priorknowledge\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def _clustering(time_series):\n",
    "    pk = priorknowledge.new_knowledge(\n",
    "        target_app=\"train-ticket\",\n",
    "        target_metric_types={\n",
    "            \"containers\": True,\n",
    "            \"services\": True,\n",
    "            \"middlewalres\": True,\n",
    "            \"nodes\": False,\n",
    "        },\n",
    "        mappings={\"nodes-containers\": {}},\n",
    "    )\n",
    "    metric_name_to_values = {metric: scipy.stats.zscore(values) for metric, values in time_series}\n",
    "    _, clustering_info = tsdr.Tsdr(\"residual_integral\", **{\n",
    "        \"step2_clustering_method_name\": \"dbscan\",\n",
    "        \"step2_dbscan_min_pts\": 2,\n",
    "        \"step2_dbscan_dist_type\": \"sbd\",  # 'pearsonr' or 'sbd'\n",
    "        \"step2_dbscan_algorithm\": \"hdbscan\",  # 'dbscan' or 'hdbscan'\n",
    "        \"step2_clustering_series_type\": \"raw\",  # 'raw', 'anomaly_score' or 'binary_anomaly_score'\n",
    "        \"step2_clustering_choice_method\": 'medoid',  # 'medoid' or 'maxsum'\n",
    "    }).reduce_multivariate_series(pd.DataFrame(metric_name_to_values), pk, n_workers=1)\n",
    "    return clustering_info\n",
    "\n",
    "clustering_infos = Parallel(n_jobs=-1)(delayed(_clustering)(ts) for ts in time_series_by_case.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chaos_type</th>\n",
       "      <th>chaos_comp</th>\n",
       "      <th>cluster_no</th>\n",
       "      <th>rep</th>\n",
       "      <th>positives</th>\n",
       "      <th>negatives</th>\n",
       "      <th>total_metrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pod-memory-hog</td>\n",
       "      <td>ts-preserve-service</td>\n",
       "      <td>1</td>\n",
       "      <td>s-ts-preserve_request_duration_seconds</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pod-memory-hog</td>\n",
       "      <td>ts-preserve-service</td>\n",
       "      <td>2</td>\n",
       "      <td>c-ts-preserve-service_fs_reads_bytes_total</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pod-memory-hog</td>\n",
       "      <td>ts-preserve-service</td>\n",
       "      <td>3</td>\n",
       "      <td>m-ts-preserve-service_java_lang_GarbageCollect...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pod-memory-hog</td>\n",
       "      <td>ts-preserve-service</td>\n",
       "      <td>4</td>\n",
       "      <td>m-ts-preserve-service_java_lang_Threading_Curr...</td>\n",
       "      <td>141</td>\n",
       "      <td>135</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pod-memory-hog</td>\n",
       "      <td>ts-preserve-service</td>\n",
       "      <td>5</td>\n",
       "      <td>m-ts-preserve-service_Tomcat_RequestProcessor_...</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>pod-network-loss</td>\n",
       "      <td>ts-price-mongo</td>\n",
       "      <td>52</td>\n",
       "      <td>m-ts-price-mongo_mongodb_top_remove_count</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>pod-network-loss</td>\n",
       "      <td>ts-price-mongo</td>\n",
       "      <td>53</td>\n",
       "      <td>m-ts-price-mongo_mongodb_ss_tcmalloc_tcmalloc_...</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>pod-network-loss</td>\n",
       "      <td>ts-price-mongo</td>\n",
       "      <td>54</td>\n",
       "      <td>m-ts-price-mongo_mongodb_sys_netstat_TcpExt_TC...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>pod-network-loss</td>\n",
       "      <td>ts-price-mongo</td>\n",
       "      <td>55</td>\n",
       "      <td>m-ts-price-mongo_mongodb_top_writeLock_time</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>pod-network-loss</td>\n",
       "      <td>ts-price-mongo</td>\n",
       "      <td>56</td>\n",
       "      <td>m-ts-price-mongo_mongodb_sys_netstat_TcpExt_TC...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>691 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           chaos_type           chaos_comp  cluster_no  \\\n",
       "0      pod-memory-hog  ts-preserve-service           1   \n",
       "1      pod-memory-hog  ts-preserve-service           2   \n",
       "2      pod-memory-hog  ts-preserve-service           3   \n",
       "3      pod-memory-hog  ts-preserve-service           4   \n",
       "4      pod-memory-hog  ts-preserve-service           5   \n",
       "..                ...                  ...         ...   \n",
       "686  pod-network-loss       ts-price-mongo          52   \n",
       "687  pod-network-loss       ts-price-mongo          53   \n",
       "688  pod-network-loss       ts-price-mongo          54   \n",
       "689  pod-network-loss       ts-price-mongo          55   \n",
       "690  pod-network-loss       ts-price-mongo          56   \n",
       "\n",
       "                                                   rep  positives  negatives  \\\n",
       "0               s-ts-preserve_request_duration_seconds          1          2   \n",
       "1           c-ts-preserve-service_fs_reads_bytes_total          3          0   \n",
       "2    m-ts-preserve-service_java_lang_GarbageCollect...         10          0   \n",
       "3    m-ts-preserve-service_java_lang_Threading_Curr...        141        135   \n",
       "4    m-ts-preserve-service_Tomcat_RequestProcessor_...         28          0   \n",
       "..                                                 ...        ...        ...   \n",
       "686          m-ts-price-mongo_mongodb_top_remove_count         10          0   \n",
       "687  m-ts-price-mongo_mongodb_ss_tcmalloc_tcmalloc_...         21          7   \n",
       "688  m-ts-price-mongo_mongodb_sys_netstat_TcpExt_TC...          3          0   \n",
       "689        m-ts-price-mongo_mongodb_top_writeLock_time          1          0   \n",
       "690  m-ts-price-mongo_mongodb_sys_netstat_TcpExt_TC...          6          0   \n",
       "\n",
       "     total_metrics  \n",
       "0                3  \n",
       "1                3  \n",
       "2                5  \n",
       "3               24  \n",
       "4                8  \n",
       "..             ...  \n",
       "686              5  \n",
       "687              8  \n",
       "688              3  \n",
       "689              2  \n",
       "690              4  \n",
       "\n",
       "[691 rows x 7 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "eval_stat: list[tuple[str, str, int, str, int, int, int]] = []\n",
    "for i, ((chaos_type, chaos_comp), time_series) in enumerate(time_series_by_case.items()):\n",
    "    clustering_info = clustering_infos[i]\n",
    "    for i, (representative_metric, sub_metrics) in enumerate(clustering_info.items(), start=1):\n",
    "        positives, negatives = 0, 0\n",
    "        for u, v in combinations([representative_metric] + sub_metrics, 2):\n",
    "            u_atype: str = samples[chaos_type, chaos_comp, u][\"anomaly_type\"]\n",
    "            v_atype: str = samples[chaos_type, chaos_comp, v][\"anomaly_type\"]\n",
    "            if u_atype == v_atype:\n",
    "                positives += 1\n",
    "            else:\n",
    "                negatives += 1\n",
    "\n",
    "        eval_stat.append((chaos_type, chaos_comp, i, representative_metric, positives, negatives, len(sub_metrics)+1))\n",
    "\n",
    "eval_df = pd.DataFrame(eval_stat, columns=[\"chaos_type\", \"chaos_comp\", \"cluster_no\", \"rep\", \"positives\", \"negatives\", \"total_metrics\"])\n",
    "eval_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c0f02f5f97634d426ffcfa502db37ef392cddba0a927ded2fc10600c3b8bead5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
